{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Voice Assistant for Your Knowledge Base\n",
    "\n",
    "• Find the link to the “[JarvisBase](https://github.com/peterw/JarvisBase)” GitHub repository for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "This tutorial focuses on voice capabilities. In this project, we will create a voice assistant that integrates OpenAI’s Whisper to convert voice inputs into text. After the transcription is complete, voice responses will be generated using Eleven Labs, a company renowned for its high-quality text-to-speech API that adeptly captures emotion and tone. Using this API will ensure that the voice assistant can communicate with users in a clear and natural tone.\n",
    "\n",
    "At the heart of this project is a question-answering system. When a question is asked, the system retrieves relevant documents from this database. These documents and the question are then processed by a large language model (LLM). The LLM utilizes this information to formulate an appropriate response.\n",
    "\n",
    "The project includes the Streamlit service to create an interactive user interface (UI), enhancing user interaction with the assistant. This basic frontend allows users to ask questions using either natural language or voice and generates responses in both text and audio formats.\n",
    "\n",
    "![image](./images/jarvis-base.jpg)\n",
    "\n",
    "Start by installing the necessary libraries for this project. While it’s best to use the most recent versions of these packages for the best results, the provided code was used with specific versions. They can be installed using the pip packages manager. A link to this requirement file is accessible at [towardsai.net/book](http://towardsai.net/book)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the API keys and tokens. They need to be set in the environment variable as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_custom_utils.helper import get_openai_api_key, get_activeloop_api_key, get_eleven_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "ACTIVELOOP_API_KEY = get_activeloop_api_key()\n",
    "ELEVEN_API_KEY = get_eleven_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Content from Hugging Face Hub\n",
    "\n",
    "We’ll begin by gathering documents from the Hugging Face Hub. These articles will form the foundation of our voice assistant’s knowledge base. We will use web scraping methods to collect relevant knowledge documents.\n",
    "\n",
    "Let’s look at and run the script.py file.\n",
    "\n",
    "Import the required modules, load environment variables, and establish the path for Deep Lake. It also creates an instance of OpenAIEmbeddings, which will be used later to embed the scraped articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import re\n",
    "\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\"\n",
    "my_activeloop_dataset_name = \"langchain_course_jarvis_assistant\"\n",
    "dataset_path= 'hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}'\n",
    "\n",
    "embeddings =  OpenAIEmbeddings(model_name=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile a list of relative URLs that lead to knowledge documents from the Hugging Face Hub. To do this, define the function `get_documentation_urls()` and attach these relative URLs to the base URL of the Hugging Face Hub using another function, `construct_full_url()`, effectively establishing full URLs that can be accessed directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documentation_urls():\n",
    "    # List of relative URLs for Hugging Face documentation pages,\n",
    "    # commented a lot of these because it would take too long to scrape\n",
    "    # all of them\n",
    "    return [\n",
    "            '/docs/huggingface_hub/guides/overview',\n",
    "            '/docs/huggingface_hub/guides/download',\n",
    "            '/docs/huggingface_hub/guides/upload',\n",
    "            '/docs/huggingface_hub/guides/hf_file_system',\n",
    "            '/docs/huggingface_hub/guides/repository',\n",
    "            '/docs/huggingface_hub/guides/search',\n",
    "            # You may add additional URLs here or replace all of them\n",
    "    ]\n",
    "\n",
    "def construct_full_url(base_url, relative_url):\n",
    "    # Construct the full URL by appending the relative URL to the base URL\n",
    "    return base_url + relative_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script compiles the gathered content from various URLs. This is executed by the `scrape_all_content()` function, which invokes the `scrape_page_content()` function for each URL. Next, the resulting text is stored in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_content(url):\n",
    "    # Send a GET request to the URL and parse the HTML response using\n",
    "    # BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Extract the desired content from the page (in this case, the body text)\n",
    "    text=soup.body.text.strip()\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', text)\n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def scrape_all_content(base_url, relative_urls, filename):\n",
    "    # Loop through the list of URLs, scrape content and add it to the\n",
    "    # content list\n",
    "    content = []\n",
    "    for relative_url in relative_urls:\n",
    "        full_url = construct_full_url(base_url, relative_url)\n",
    "        scraped_content = scrape_page_content(full_url)\n",
    "        content.append(scraped_content.rstrip('\\n'))\n",
    "\n",
    "    # Write the scraped content to a file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for item in content:\n",
    "            file.write(\"%s\\n\" % item)\n",
    "    \n",
    "    return content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-building-llm-powered-applications-with-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
