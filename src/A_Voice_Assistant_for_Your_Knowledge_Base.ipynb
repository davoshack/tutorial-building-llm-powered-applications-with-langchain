{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Voice Assistant for Your Knowledge Base\n",
    "\n",
    "• Find the link to the “[JarvisBase](https://github.com/peterw/JarvisBase)” GitHub repository for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "This tutorial focuses on voice capabilities. In this project, we will create a voice assistant that integrates OpenAI’s Whisper to convert voice inputs into text. After the transcription is complete, voice responses will be generated using Eleven Labs, a company renowned for its high-quality text-to-speech API that adeptly captures emotion and tone. Using this API will ensure that the voice assistant can communicate with users in a clear and natural tone.\n",
    "\n",
    "At the heart of this project is a question-answering system. When a question is asked, the system retrieves relevant documents from this database. These documents and the question are then processed by a large language model (LLM). The LLM utilizes this information to formulate an appropriate response.\n",
    "\n",
    "The project includes the Streamlit service to create an interactive user interface (UI), enhancing user interaction with the assistant. This basic frontend allows users to ask questions using either natural language or voice and generates responses in both text and audio formats.\n",
    "\n",
    "![image](./images/jarvis-base.jpg)\n",
    "\n",
    "Start by installing the necessary libraries for this project. While it’s best to use the most recent versions of these packages for the best results, the provided code was used with specific versions. They can be installed using the pip packages manager. A link to this requirement file is accessible at [towardsai.net/book](http://towardsai.net/book)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the API keys and tokens. They need to be set in the environment variable as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_custom_utils.helper import get_openai_api_key, get_activeloop_api_key, get_eleven_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "ACTIVELOOP_API_KEY = get_activeloop_api_key()\n",
    "ELEVEN_API_KEY = get_eleven_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Content from Hugging Face Hub\n",
    "\n",
    "We’ll begin by gathering documents from the Hugging Face Hub. These articles will form the foundation of our voice assistant’s knowledge base. We will use web scraping methods to collect relevant knowledge documents.\n",
    "\n",
    "Let’s look at and run the script.py file.\n",
    "\n",
    "Import the required modules, load environment variables, and establish the path for Deep Lake. It also creates an instance of OpenAIEmbeddings, which will be used later to embed the scraped articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import re\n",
    "\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\"\n",
    "my_activeloop_dataset_name = \"langchain_course_jarvis_assistant\"\n",
    "dataset_path= 'hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}'\n",
    "\n",
    "embeddings =  OpenAIEmbeddings(model_name=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile a list of relative URLs that lead to knowledge documents from the Hugging Face Hub. To do this, define the function `get_documentation_urls()` and attach these relative URLs to the base URL of the Hugging Face Hub using another function, `construct_full_url()`, effectively establishing full URLs that can be accessed directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documentation_urls():\n",
    "    # List of relative URLs for Hugging Face documentation pages,\n",
    "    # commented a lot of these because it would take too long to scrape\n",
    "    # all of them\n",
    "    return [\n",
    "            '/docs/huggingface_hub/guides/overview',\n",
    "            '/docs/huggingface_hub/guides/download',\n",
    "            '/docs/huggingface_hub/guides/upload',\n",
    "            '/docs/huggingface_hub/guides/hf_file_system',\n",
    "            '/docs/huggingface_hub/guides/repository',\n",
    "            '/docs/huggingface_hub/guides/search',\n",
    "            # You may add additional URLs here or replace all of them\n",
    "    ]\n",
    "\n",
    "def construct_full_url(base_url, relative_url):\n",
    "    # Construct the full URL by appending the relative URL to the base URL\n",
    "    return base_url + relative_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script compiles the gathered content from various URLs. This is executed by the `scrape_all_content()` function, which invokes the `scrape_page_content()` function for each URL. Next, the resulting text is stored in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_content(url):\n",
    "    # Send a GET request to the URL and parse the HTML response using\n",
    "    # BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Extract the desired content from the page (in this case, the body text)\n",
    "    text=soup.body.text.strip()\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', text)\n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def scrape_all_content(base_url, relative_urls, filename):\n",
    "    # Loop through the list of URLs, scrape content and add it to the\n",
    "    # content list\n",
    "    content = []\n",
    "    for relative_url in relative_urls:\n",
    "        full_url = construct_full_url(base_url, relative_url)\n",
    "        scraped_content = scrape_page_content(full_url)\n",
    "        content.append(scraped_content.rstrip('\\n'))\n",
    "\n",
    "    # Write the scraped content to a file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for item in content:\n",
    "            file.write(\"%s\\n\" % item)\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Splitting Texts\n",
    "\n",
    "To prepare the gathered text into our vector database, the content is first retrieved from the file using the `load_docs()` function, which separates it into distinct documents. These documents are then divided into smaller segments using the `split_docs()` function.\n",
    "\n",
    "The command `text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)` initializes a text splitter designed to segment the text into character-based chunks. It divides the documents into sections of roughly 1000 characters with no overlapping content in the consecutive sections within docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load documents from a file\n",
    "def load_docs(root_dir, filename):\n",
    "    # Create an empty list to hold the documents\n",
    "    docs = []\n",
    "    try:\n",
    "        # Load the file using the TextLoader class and UTF-8 encoding\n",
    "        loader = TextLoader(os.path.join(\n",
    "            root_dir, filename), encoding='utf-8')\n",
    "        # Split the loaded file into separate documents and add them to the list\n",
    "        # of documents\n",
    "        docs.extend(loader.load_and_split())\n",
    "    except Exception as e:\n",
    "        # If an error occurs during loading, ignore it and return an empty list\n",
    "        # of documents\n",
    "        pass\n",
    "    # Return the list of documents\n",
    "    return docs\n",
    "  \n",
    "def split_docs(docs):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding and Storing in Deep Lake\n",
    "\n",
    "The next phase is embedding the articles and storing them in Deep Lake.\n",
    "\n",
    "The following code sets up a Deep Lake instance, specifying the dataset path and the **OpenAIEmbeddings** function as an embedding function to use. The **OpenAIEmbeddings** function transforms the text segments into their embedding vectors, a format compatible with the vector database. With the `.add_documents` method, the texts are processed and stored within the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main function\n",
    "def main():\n",
    "    base_url = 'https://huggingface.co'\n",
    "    # Set the name of the file to which the scraped content will be saved\n",
    "    filename='content.txt'\n",
    "    # Set the root directory where the content file will be saved\n",
    "    root_dir ='./'\n",
    "    relative_urls = get_documentation_urls()\n",
    "    # Scrape all the content from the relative URLs and save it to the content\n",
    "    # file\n",
    "    content = scrape_all_content(base_url, relative_urls, filename)\n",
    "    # Load the content from the file\n",
    "    docs = load_docs(root_dir, filename)\n",
    "    # Split the content into individual documents\n",
    "    texts = split_docs(docs)\n",
    "    # Create a DeepLake database with the given dataset path and embedding\n",
    "    # function\n",
    "    db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "    # Add the individual documents to the database\n",
    "    db.add_documents(texts)\n",
    "    # Clean up by deleting the content file\n",
    "    os.remove(filename)\n",
    "\n",
    "# Call the main function if this script is being run as the main program\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps are organized within the main function. It establishes the required parameters, activates the outlined functions, and manages the entire procedure, from scraping web content to integrating it into the Deep Lake database. It also removes the content file, ensuring a clean workspace."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-building-llm-powered-applications-with-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
