{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A YouTube Video Summarizer Using Whisper and LangChain\n",
    "\n",
    "• Find the  [Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2007%20-%20Create_a_YouTube_Video_Summarizer_Using_Whisper_and_LangChain_.ipynb)  for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "The project involves a series of steps, starting with downloading the audio file from YouTube. Once the audio file is obtained, it is transcribed using **Whisper**. After the transcription is complete, the text is summarized using LangChain, employing three different approaches: *stuff*, *refine*, and *map_reduce*. Finally, multiple transcriptions are added to the DeepLake database to enable question-answering for those videos.\n",
    "\n",
    "The following diagram explains what we are going to do in this project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./youtube_video_summarizer.jpg)\n",
    "\n",
    "*Our YouTube video summarizer pipeline.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, start by installing the packages using the command: `!pip install langchain==0.0.208 deeplake openai==0.27.8 tiktoken,  yt_dlp, and openai-whisper.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, install [ffmpeg](https://ffmpeg.org/); it is a prerequisite for the ***yt_dlp*** package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, add the API key for OpenAI and Deep Lake services to the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_custom_utils.helper import get_openai_api_key, get_deeplake_api_key, print_response\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "DEEPLAKE_API_KEY = get_deeplake_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial teaches how to programmatically summarize a video featuring Yann LeCun, a notable computer scientist and AI researcher. The video covers LeCun’s thoughts on the challenges associated with large language models. However, the code would work with any other video as long as it can be summarized using only its audio (as the model won’t know what is shown in the video) and that ideally contains only a few speakers. Video podcasts are ideal for this project.\n",
    "\n",
    "The download_mp4_from_youtube() function downloads the highest quality mp4 video file from a given YouTube link and saves it to a specified path and filename. To use this function, simply copy and paste the URL of the chosen video into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "def download_mp4_from_youtube(url):\n",
    "    # Set the options for the download\n",
    "    filename = 'lecuninterview.mp4'\n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
    "        'outtmpl': filename,\n",
    "        'quiet': True,\n",
    "    }\n",
    "\n",
    "    # Download the video file\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(url, download=True)\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=mBjPyte2ZZo\"\n",
    "download_mp4_from_youtube(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the video MP4 has been downloaded, the next step is to transcribe its audio using a speech-to-text model. One of the currently most popular open-source speech-to-text models is OpenAI’s Whisper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing Audio with Whisper\n",
    "\n",
    "***Whisper*** is an advanced automatic speech recognition system developed by OpenAI. It’s trained on a dataset of 680,000 hours of multilingual and multitasking supervised data from the web. This extensive and diverse dataset contributes to the system’s ability to efficiently manage accents, background noise, and technical jargon.\n",
    "\n",
    "The previously installed whisper package includes the `.load_model()` method, which downloads the model and transcribes a video file. Several models are available: tiny, base, small, medium, and large, for balancing accuracy and processing speed. We will use the 'base' model for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"lecuninterview.mp4\")\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> /home/cloudsuperadmin/.local/lib/python3.9/site-packages/whisper/transcribe.py:114:\n",
    "> UserWarning: FP16 is not supported on CPU; using FP32 instead  \n",
    "> warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")     \n",
    "> Hi, I'm Craig Smith, and this is I on A On. This week I talked to Jan\n",
    "> LeCoon, one of the seminal figures in deep learning development and a\n",
    "> long-time proponent of self-supervised learning. Jan spoke about\n",
    "> what's missing in large language models and his new joint embedding\n",
    "> predictive architecture which may be a step toward filling that gap.\n",
    "> He also talked about his theory of consciousness and the potential for\n",
    "> AI systems to someday exhibit the features of consciousness. It's a\n",
    "> fascinating conversation that I hope you'll enjoy. Okay, so Jan, it's\n",
    "> great to see you again. I wanted to talk to you about where you've\n",
    "> gone with so supervised learning since last week's spoke. In\n",
    "> particular, I'm interested in how it relates to large language models\n",
    "> because they have really come on stream since we spoke. In fact, in\n",
    "> your talk about JEPA, which is joint embedding predictive\n",
    "> architecture. […and so on]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is generated as raw text and can be saved to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('text.txt', 'w') as file:  \n",
    "    file.write(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the transcription is ready, the next step is to split it into chunks using a text splitter and then use the chunks to generate a summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Text and Generating the Summary\n",
    "\n",
    "Import the necessary classes and utilities from the LangChain library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, LLMChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates an instance of the `RecursiveCharacterTextSplitter` class. This class is used to split input text into more manageable, smaller segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RecursiveCharacterTextSplitter` is set up with a `chunk_size` of 1000 characters, without any `chunk_overlap`, and uses spaces, commas, and newline characters as separators.\n",
    "\n",
    "Now, open the previously saved text file and use the `.split_text()` method to segment the transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "with open('text.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "docs = [Document(page_content=t) for t in texts[:4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Document object is initialized with the content of a chunk from the texts list. The `[:4]` slice notation indicates that only the first four chunks will be used to create the Document objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import textwrap\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Craig Smith interviews Jan LeCoon, a deep learning developer and\n",
    "> proponent of self-supervised learning, about his new joint embedding\n",
    "> predictive architecture and his theory of consciousness. Jan's\n",
    "> research focuses on self-supervised learning and its use for\n",
    "> pre-training transformer architectures, which are used to predict\n",
    "> missing words in a piece of text. Additionally, large language models\n",
    "> are used to predict the next word in a sentence, but it is difficult\n",
    "> to represent uncertain predictions when applying this to video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">💡The textwrap library in Python provides a convenient way to wrap and format plain text by adjusting line breaks in an input paragraph. It is particularly useful when displaying text within a limited width, such as in console outputs, emails, or other formatted text displays. The library includes convenience functions like `wrap`, `fill`, and `shorten`, as well as the `TextWrapper` class that handles most of the work. If you’re curious, find more information on [Text wrapping and filling](https://docs.python.org/3/library/textwrap.html) at [towardsai.net/book](http://towardsai.net/book)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows the prompt template used with the `map_reduce` chain type. The `map-reduce` process first summarizes each document separately using a language model (Map step), turning each into a new document. Then, it combines all of them into one document (Reduce step) to form the final summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\n CONCISE SUMMARY:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"stuff\" approach involves using all text from the transcribed video in a single prompt, which is a basic and straightforward method. However, it might not be the most efficient for handling large volumes of text, and it may overflow the maximum context size of the LLM. It’s always advised to monitor the number of tokens used in the prompts because different LLM APIs may behave differently when the maximum token limit is exceeded. Some APIs may throw errors, while others may silently use only the first part of the prompt until it fills the LLM context size completely, thus generating an output with an incomplete prompt.\n",
    "\n",
    "To generate the summary for this tutorial, we’re going to experiment with the prompt below, which will output the summary as bullet points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise bullet point summary of the following:\n",
    "\n",
    "{text}\n",
    "\n",
    "CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n",
    "\n",
    "BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template,\n",
    "                        input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also initialized the summarization chain using the stuff as `chain_type` and the prompt above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm,\n",
    "                             chain_type=\"stuff\",\n",
    "                             prompt=BULLET_POINT_PROMPT)\n",
    "\n",
    "output_summary = chain.run(docs)\n",
    "\n",
    "wrapped_text = textwrap.fill(output_summary,\n",
    "                             width=1000,\n",
    "                             break_long_words=False,\n",
    "                             replace_whitespace=False)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Jan LeCoon is a seminal figure in deep learning development and a long time proponent of self-supervised learning  \n",
    "> - Discussed his new joint embedding predictive architecture which may be a step toward filling the gap in large language models  \n",
    "> - Theory of consciousness and potential for AI systems to exhibit features of consciousness  \n",
    "> - Self-supervised learning revolutionized natural language processing  \n",
    "> - Large language models lack a world model and are generative models, making it difficult to represent uncertain predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LangChain** provides the flexibility to create custom prompts tailored to specific needs. For instance, if the objective is to receive a summarization output in French, one can construct a prompt instructing the language model to generate a summary in French.\n",
    "\n",
    "The 'refine' summarization chain is an approach designed to generate more precise and *context-sensitive summaries*. This method follows an iterative process to enhance the summary by incorporating additional context as needed. In practice, it initiates by summarizing the first text chunk. Subsequently, the generated summary is enriched with new information from each subsequent chunk. It can produce more accurate and context-aware summaries than chains like '*stuff*' and '*map_reduce*', at the cost of more LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "output_summary = chain.run(docs)\n",
    "wrapped_text = textwrap.fill(output_summary, width=100)\n",
    "print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Craig Smith interviews Jan LeCoon, a deep learning developer and proponent of self-supervised learning, about his new joint embedding predictive architecture and his theory of consciousness. Jan discusses the gap in large language models and the potential for AI systems to exhibit features of consciousness. He explains how self-supervised learning has revolutionized natural language processing through the use of transformer architectures for pre-training, such as taking a piece of text, removing some of the words, and replacing them with black markers to train a large neural net to predict the words that are missing. This technique has been used in practical applications such as contact moderation systems on Facebook, Google, YouTube, and more. Jan also explains how this technique can be used to represent uncertain predictions in generative models, such as predicting the missing words in a text, or predicting the missing frames in a video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Transcripts to Deep Lake\n",
    "\n",
    "Now, let’s continue by transcribing more videos, storing the transcriptions in the Deep Lake database, and retrieving information via the QA chain.\n",
    "\n",
    "First, we need to make slight modifications to the video downloading script to enable it to work with a list of URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "def download_mp4_from_youtube(urls, job_id):\n",
    "    # This will hold the titles and authors of each downloaded video\n",
    "    video_info = []\n",
    "\n",
    "    for i, url in enumerate(urls):\n",
    "        # Set the options for the download\n",
    "        file_temp = f'./{job_id}_{i}.mp4'\n",
    "        ydl_opts = {\n",
    "            'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
    "            'outtmpl': file_temp,\n",
    "            'quiet': True,\n",
    "        }\n",
    "\n",
    "        # Download the video file\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            result = ydl.extract_info(url, download=True)\n",
    "            title = result.get('title', \"\")\n",
    "            author = result.get('uploader', \"\")\n",
    "\n",
    "        # Add the title and author to our list\n",
    "        video_info.append((file_temp, title, author))\n",
    "\n",
    "    return video_info\n",
    "\n",
    "urls=[\n",
    "    \"https://www.youtube.com/watch?v=mBjPyte2ZZo&t=78s\",\n",
    "    \"https://www.youtube.com/watch?v=cjs7QKJNVYM\",\n",
    "]\n",
    "vides_details = download_mp4_from_youtube(urls, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transcribe the videos using **Whisper** as we previously saw and save the results in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "# load the model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# iterate through each video and transcribe\n",
    "results = []\n",
    "for video in vides_details:\n",
    "    result = model.transcribe(video[0])\n",
    "    results.append( result['text'] )\n",
    "    print(f\"Transcription for {video[0]}:\\n{result['text']}\\n\")\n",
    "\n",
    "with open ('text.txt', 'w') as file:  \n",
    "    file.write(results['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Transcription for ./1_0.mp4:  \n",
    "Hi, I'm Craig Smith and this is I on A On. This week I talk to Jan LeCoon, one of the seminal figures in deep learning development and a long time proponent of self-supervised learning. Jan spoke about what's missing in large language models and about his new joint embedding predictive architecture which may be a step toward filling that gap. He also talked about his theory of consciousness and the potential for AI systems to someday exhibit the features of consciousness..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the texts from the file and use the text splitter to split the text into chunks with zero overlap before storing them in Deep Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the texts\n",
    "with open('text.txt') as f:\n",
    "    text = f.read()\n",
    "texts = text_splitter.split_text(text)\n",
    "\n",
    "# Split the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0, separators=[\" \", \",\", \"\\n\"]\n",
    "    )\n",
    "texts = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pack all the chunks into a Document LangChain object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = [Document(page_content=t) for t in texts[:4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Deep Lake and build a database with embedded documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\"\n",
    "my_activeloop_dataset_name = \"langchain_course_youtube_summarizer\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the information from the database, we need to construct a retriever object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['k'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `distance_metric` parameter defines how the Retriever computes similarity or “distance” between data points in the database. By setting this parameter to 'cos', cosine similarity is employed as the distance metric. ***Cosine similarity***, a standard measure in information retrieval, evaluates the similarity between two non-zero vectors in a vector space by measuring the cosine of the angle between them. This metric is frequently used to assess the similarity between documents or text segments. Additionally, setting 'k' to 4 instructs the Retriever to return the four most similar results based on the distance metric.\n",
    "\n",
    "We can also create a custom prompt template to use within the ***QA chain***. The `RetrievalQA` chain queries similar contents from the database, using the retrieved records as context for answering questions. Custom prompts allow for tailored tasks, such as retrieving documents and summarizing the output in a bullet point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"Use the following pieces of transcripts from a video to answer the question in bullet points and summarized. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Summarized answer in bullter points:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the custom prompt using the `chain_type_kwargs` argument and the `stuff` variation as the ***chain type***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=retriever,\n",
    "                                 chain_type_kwargs=chain_type_kwargs)\n",
    "\n",
    "print(qa.run(\"Summarize the mentions of google according to their AI program\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> • Google has developed an AI program to help people with their\n",
    "> everyday tasks.   • The AI program can be used to search for\n",
    "> information, make recommendations, and provide personalized\n",
    "> experiences.   • Google is using AI to improve its products and\n",
    "> services, such as Google Maps and Google Assistant.   • Google is also\n",
    "> using AI to help with medical research and to develop new\n",
    "> technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can always change the prompt and experiment with different types of chains to discover the best combination for your project’s needs and limits.\n",
    "\n",
    "_We've created this lesson by adapting the code from_ [github.com/idontcalculate/langchain](https://github.com/idontcalculate/langchain/blob/main/yt-whisper-sum.ipynb)_._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-building-llm-powered-applications-with-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
