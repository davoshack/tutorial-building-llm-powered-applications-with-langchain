{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A YouTube Video Summarizer Using Whisper and LangChain\n",
    "\n",
    "• Find the  [Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2007%20-%20Create_a_YouTube_Video_Summarizer_Using_Whisper_and_LangChain_.ipynb)  for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "The project involves a series of steps, starting with downloading the audio file from YouTube. Once the audio file is obtained, it is transcribed using **Whisper**. After the transcription is complete, the text is summarized using LangChain, employing three different approaches: *stuff*, *refine*, and *map_reduce*. Finally, multiple transcriptions are added to the DeepLake database to enable question-answering for those videos.\n",
    "\n",
    "The following diagram explains what we are going to do in this project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./youtube_video_summarizer.jpg)\n",
    "\n",
    "*Our YouTube video summarizer pipeline.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, start by installing the packages using the command: `!pip install langchain==0.0.208 deeplake openai==0.27.8 tiktoken,  yt_dlp, and openai-whisper.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, install [ffmpeg](https://ffmpeg.org/); it is a prerequisite for the ***yt_dlp*** package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, add the API key for OpenAI and Deep Lake services to the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_custom_utils.helper import get_openai_api_key, get_deeplake_api_key, print_response\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "DEEPLAKE_API_KEY = get_deeplake_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial teaches how to programmatically summarize a video featuring Yann LeCun, a notable computer scientist and AI researcher. The video covers LeCun’s thoughts on the challenges associated with large language models. However, the code would work with any other video as long as it can be summarized using only its audio (as the model won’t know what is shown in the video) and that ideally contains only a few speakers. Video podcasts are ideal for this project.\n",
    "\n",
    "The download_mp4_from_youtube() function downloads the highest quality mp4 video file from a given YouTube link and saves it to a specified path and filename. To use this function, simply copy and paste the URL of the chosen video into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "def download_mp4_from_youtube(url):\n",
    "    # Set the options for the download\n",
    "    filename = 'lecuninterview.mp4'\n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]',\n",
    "        'outtmpl': filename,\n",
    "        'quiet': True,\n",
    "    }\n",
    "\n",
    "    # Download the video file\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        result = ydl.extract_info(url, download=True)\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=mBjPyte2ZZo\"\n",
    "download_mp4_from_youtube(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the video MP4 has been downloaded, the next step is to transcribe its audio using a speech-to-text model. One of the currently most popular open-source speech-to-text models is OpenAI’s Whisper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing Audio with Whisper\n",
    "\n",
    "***Whisper*** is an advanced automatic speech recognition system developed by OpenAI. It’s trained on a dataset of 680,000 hours of multilingual and multitasking supervised data from the web. This extensive and diverse dataset contributes to the system’s ability to efficiently manage accents, background noise, and technical jargon.\n",
    "\n",
    "The previously installed whisper package includes the `.load_model()` method, which downloads the model and transcribes a video file. Several models are available: tiny, base, small, medium, and large, for balancing accuracy and processing speed. We will use the 'base' model for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"lecuninterview.mp4\")\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> /home/cloudsuperadmin/.local/lib/python3.9/site-packages/whisper/transcribe.py:114:\n",
    "> UserWarning: FP16 is not supported on CPU; using FP32 instead  \n",
    "> warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")     \n",
    "> Hi, I'm Craig Smith, and this is I on A On. This week I talked to Jan\n",
    "> LeCoon, one of the seminal figures in deep learning development and a\n",
    "> long-time proponent of self-supervised learning. Jan spoke about\n",
    "> what's missing in large language models and his new joint embedding\n",
    "> predictive architecture which may be a step toward filling that gap.\n",
    "> He also talked about his theory of consciousness and the potential for\n",
    "> AI systems to someday exhibit the features of consciousness. It's a\n",
    "> fascinating conversation that I hope you'll enjoy. Okay, so Jan, it's\n",
    "> great to see you again. I wanted to talk to you about where you've\n",
    "> gone with so supervised learning since last week's spoke. In\n",
    "> particular, I'm interested in how it relates to large language models\n",
    "> because they have really come on stream since we spoke. In fact, in\n",
    "> your talk about JEPA, which is joint embedding predictive\n",
    "> architecture. […and so on]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is generated as raw text and can be saved to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('text.txt', 'w') as file:  \n",
    "    file.write(result['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the transcription is ready, the next step is to split it into chunks using a text splitter and then use the chunks to generate a summary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-building-llm-powered-applications-with-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
