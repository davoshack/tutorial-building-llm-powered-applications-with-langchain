{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Prompts and Example Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Find the  [Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2006%20-%20Getting_the_Best_of_Few_Shot_Prompts_and_Example_Selectors.ipynb)  for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "We’ll cover how few-shot prompts and example selectors can enhance the performance of language models in LangChain. While various methods can be used to implement  **few-shot prompting**  and  **example selectors**  in LangChain, we’ll discuss three distinct approaches, examining their advantages and disadvantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_custom_utils.helper import get_openai_api_key, get_activeloop_api_key, print_response\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "ACTIVELOOP_API_KEY = get_activeloop_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Human/AI Messages\n",
    "\n",
    "Using few-shot prompting with alternating human and AI messages is particularly useful for chat-based applications. This technique requires the language model to understand the conversational context and respond appropriately.\n",
    "\n",
    "Although this strategy is effective in managing conversational contexts and straightforward to implement, its flexibility is limited to chat-based applications. Despite this, alternating human/AI messages can be creatively employed. In this approach, you are essentially writing the chatbot’s responses in your own words and using them as input for the model.\n",
    "\n",
    "For example, we can create a chat prompt that translates English into pirate language by showing an example to the model using `AIMessagePromptTemplate`. Below is a code snippet illustrating it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "template=\"You are a helpful assistant that translates english to pirate.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "example_human = HumanMessagePromptTemplate.from_template(\"Hi\")\n",
    "example_ai = AIMessagePromptTemplate.from_template(\"Argh me mateys\")\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, example_human, example_ai, human_message_prompt])\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(\"I love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting\n",
    "\n",
    "Few-shot prompting can improve the output quality as the model better understands the task by reviewing the examples. However, using more tokens might lead to less effective results if the examples provided are not carefully chosen or are misleading.\n",
    "\n",
    "Implementing the few-shot learning technique involves using the `FewShotPromptTemplate` class, which requires a `PromptTemplate` and a set of few-shot examples. The class combines the prompt template with these examples, aiding the language model in producing more accurate responses. LangChain’s `FewShotPromptTemplate` can be used to organize the approach systematically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
    "    }, {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create an example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a template, we pass the example and user query to get the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
    "chain.run(\"What's the secret to happiness?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach provides enhanced control over the **formatting** of examples and is adaptable to various applications. However, it requires manual curation of few-shot examples and may become less efficient when dealing with many examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Selectors\n",
    "\n",
    "An  **example selector**  is a tool that facilitates the selection of examples to add to a few-shot learning prompt. The core objective of few-shot learning is to develop a function that assesses the similarities between classes in the examples and query sets. An example selector can be strategically designed to pick relevant examples accurately reflecting the desired output.\n",
    "\n",
    "The `ExampleSelector` is crucial in selecting a subset of examples most beneficial for the language model. This selection process helps craft a prompt more likely to produce a high-quality response. The `LengthBasedExampleSelector` is particularly valuable when managing the context window’s length based on the user’s question length. It chooses fewer examples for longer queries and more for shorter ones, ensuring an efficient use of the available context.\n",
    "\n",
    "This section demonstrates how to use the `LengthBasedExampleSelector` class to efficiently manage the context window and customize prompts with relevant examples.\n",
    "\n",
    "First, import the required classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your examples and the `example_prompt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "    {\"word\": \"energetic\", \"antonym\": \"lethargic\"},\n",
    "    {\"word\": \"sunny\", \"antonym\": \"gloomy\"},\n",
    "    {\"word\": \"windy\", \"antonym\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of `LengthBasedExampleSelector`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=example,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `FewShotPromptTemplate` using the `example_selector` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a sample prompt using the format method to inspect the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dynamic_prompt.format(input=\"black\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method effectively handles several examples and provides customization options through different selectors. However, it requires manual curation of examples, which may only be suitable for some applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of LangChain’s `SemanticSimilarityExampleSelector` to choose examples based on their semantic similarity to the input query. This example demonstrates the steps to create an `ExampleSelector` and formulate a prompt using a few-shot methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Create a PromptTemplate\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "\n",
    "# Define some examples\n",
    "examples = [\n",
    "    {\"input\": \"0°C\", \"output\": \"32°F\"},\n",
    "    {\"input\": \"10°C\", \"output\": \"50°F\"},\n",
    "    {\"input\": \"20°C\", \"output\": \"68°F\"},\n",
    "    {\"input\": \"30°C\", \"output\": \"86°F\"},\n",
    "    {\"input\": \"40°C\", \"output\": \"104°F\"},\n",
    "]\n",
    "\n",
    "# create Deep Lake dataset\n",
    "my_activeloop_org_id = \"jdhernandez\" # TODO: use your organization id here\n",
    "my_activeloop_dataset_name = \"langchain_course_fewshot_selector\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, overwrite=True)\n",
    "\n",
    "# Embedding function\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Instantiate SemanticSimilarityExampleSelector using the examples\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples, embeddings, db, k=1\n",
    ")\n",
    "\n",
    "# Create a FewShotPromptTemplate using the example_selector\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Convert the temperature from Celsius to Fahrenheit\",\n",
    "    suffix=\"Input: {temperature}\\nOutput:\",\n",
    "    input_variables=[\"temperature\"],\n",
    ")\n",
    "\n",
    "# Test the similar_prompt with different inputs\n",
    "print(similar_prompt.format(temperature=\"10°C\"))   # Test with an input\n",
    "print(similar_prompt.format(temperature=\"30°C\"))  # Test with another input\n",
    "\n",
    "# Add a new example to the SemanticSimilarityExampleSelector\n",
    "similar_prompt.example_selector.add_example({\"input\": \"50°C\", \"output\": \"122°F\"})\n",
    "print(similar_prompt.format(temperature=\"40°C\")) # Test with a new input after adding the example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SemanticSimilarityExampleSelector` employs the Deep Lake vector database and `OpenAIEmbeddings` to assess semantic similarity. This tool stores examples in a cloud-based database (e.g., Deep Lake) and retrieves semantically similar samples.\n",
    "\n",
    "In our process, we first constructed a prompt template and included several examples related to temperature conversions. Following this, we initiated the semantic similarity example selector and created a few-shot prompt template using the selector, `example_prompt`, and the designated prefix and suffix. By utilizing the semantic similarity example selector in combination with the few-shot prompt template, we created dynamic and task-specific context-aware prompts. These tools offer a flexible and adaptable way to generate prompts, enabling the use of language models for a diverse range of tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-building-llm-powered-applications-with-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
