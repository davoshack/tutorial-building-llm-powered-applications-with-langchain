{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Our News Articles Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will improve the previously developed “News Article Summarizer” script. The goal is to improve its accuracy in extracting and presenting key information from long news articles in a bulleted list format.\n",
    "\n",
    "To achieve this, we will adapt our current summarizer to prompt the underlying language model to produce summaries as bulleted lists using output parsers. This requires specific adjustments to the framing of our prompts.\n",
    "\n",
    "Here’s a recap of what we did and what we are going to do in this project:\n",
    "\n",
    "![image](./pipeline-news-articles-summarizer.jpg)\n",
    "\n",
    "*Pipeline for our news articles summarizer with scraping, parsing, prompting, and generation.*\n",
    "\n",
    "To improve the article summarizer, we employ few-shot learning to show the model how the output should be structured in advance, helping it adapt to the desired format. We also pass the output generated by the model through output parsers to ensure that the structure of the output adheres to the desired format.\n",
    "\n",
    "This entire process, leveraging **few-shot learning** for accuracy and output parsers for format control, ensures a high-quality, structured summarization of the news articles.\n",
    "\n",
    "The initial phases of this process are technically identical to [part 1](./build-a-news-articles-summarizer.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_custom_utils.helper import get_openai_api_key, print_response\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract & parse news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "\n",
    "article_url = \"https://www.artificialintelligence-news.com/2022/01/25/meta-claims-new-ai-supercomputer-will-set-records/\"\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "\n",
    "try:\n",
    "    response = session.get(article_url, headers=headers, timeout=10)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        print(f\"Title: {article.title}\")\n",
    "        print(f\"Text: {article.text}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch article at {article_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while fetching article at {article_url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will incorporate examples into a prompt using the `FewShotPromptTemplate approach`. When applied, it will guide the model in producing a bullet list that briefly summarizes the content of the provided article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "# we get the article data from the scraping part\n",
    "article_title = article.title\n",
    "article_text = article.text\n",
    "\n",
    "# prepare template for prompt\n",
    "template = \"\"\"\n",
    "As an advanced AI, you've been tasked to summarize online articles into bulleted points. Here are a few examples of how you've done this in the past:\n",
    "\n",
    "Example 1:\n",
    "Original Article: 'The Effects of Climate Change\n",
    "Summary:\n",
    "- Climate change is causing a rise in global temperatures.\n",
    "- This leads to melting ice caps and rising sea levels.\n",
    "- Resulting in more frequent and severe weather conditions.\n",
    "\n",
    "Example 2:\n",
    "Original Article: 'The Evolution of Artificial Intelligence\n",
    "Summary:\n",
    "- Artificial Intelligence (AI) has developed significantly over the past decade.\n",
    "- AI is now used in multiple fields such as healthcare, finance, and transportation.\n",
    "- The future of AI is promising but requires careful regulation.\n",
    "\n",
    "Now, here's the article you need to summarize:\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Please provide a summarized version of the article in a bulleted list format.\n",
    "\"\"\"\n",
    "\n",
    "# format prompt\n",
    "prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "\n",
    "messages = [HumanMessage(content=prompt)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples give the model a better understanding of the expected response. Here, we need a couple of essential components:\n",
    "\n",
    "-   **Article**: Collecting the title and text of the article. These elements serve as the primary inputs for the model.\n",
    "-   **Template**: Crafting a detailed template for the prompt. This template adopts a few-shot learning approach, providing the model with examples of articles summarized into bullet lists. Additionally, it contains placeholders for the actual article title and text, which will be summarized. Subsequently, these placeholders (`{article_title}` and `{article_text}`) are replaced with the real title and text of the article using the `.format()` method.\n",
    "\n",
    "The next step involves employing the `ChatOpenAI` class to load the GPT-4 model, which creates the summary. The prompt is then fed to the language model as input. The `ChatOpenAI` class’s instance receives a `HumanMessage` list as its input argument, facilitating the generation of the desired output.\n",
    "\n",
    "The examples here involve several key components that enhance the model’s response accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# load the model\n",
    "chat = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate summary\n",
    "summary = chat(messages)\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key objective of this strategy is to incorporate a few-shot learning style within the prompt. This technique provides the model with examples that demonstrate the ideal task execution. It is possible to adapt the model’s output to meet various objectives and adhere to a specific format, tone, and style criteria by changing the prompt and examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-building-llm-powered-applications-with-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
