{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Outputs with Output Parsers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a production setting, outputs from language models in a predictable data structure are often desirable. Consider, for instance, developing a thesaurus application to generate a collection of alternative words relevant to the given context. Large language models (LLMs) can generate numerous suggestions for synonyms or similar terms. Below is an example of output from ChatGPT listing several words closely related to â€œbehavior.â€\n",
    "\n",
    "    Here are some substitute words for \"behavior\":  \n",
    "      \n",
    "    Conduct  \n",
    "    Manner  \n",
    "    Demeanor  \n",
    "    Attitude  \n",
    "    Disposition  \n",
    "    Deportment  \n",
    "    Etiquette  \n",
    "    Protocol  \n",
    "    Performance  \n",
    "    Actions\n",
    "\n",
    "The challenge arises from the absence of a dynamic method to extract relevant information from the provided text. Consider splitting the response by new lines and disregarding the initial lines. However, this approach is unreliable as thereâ€™s no assurance that responses will maintain a consistent format. The list might be numbered, or it might not include an introductory line.\n",
    "\n",
    "Output Parsers enable us to define a data structure that precisely describes what is expected from the model. In a word suggestion application, you might request a list of words or a combination of different variables, such as a word and an explanation.\n",
    "\n",
    "Structured outputs can also be enforced through APIs, such as those provided by OpenAI models, where the model can be prompted to generate outputs following a predefined schema. For instance, you can specify a JSON schema or use a Pydantic model to ensure that the outputs conform to the expected structure, making it easier to integrate into applications that require predictable data formats. This capability will be covered in more detail in the book, where we will explore practical methods to structure and validate outputs using these techniques.\n",
    "\n",
    "The Pydantic parser is versatile and has three unique types. However, other options are also available for less complex tasks.\n",
    "\n",
    "**Note:**  The thesaurus application will serve as a practical example to clarify the nuances of each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PydanticOutputParser\n",
    "\n",
    "This class instructs the model to produce its output in JSON format. The parserâ€™s output can be treated as a list, allowing for simple indexing of the results and eliminating formatting issues.\n",
    "\n",
    "```\n",
    "ðŸ’¡It is important to note that not all models have the same capability to generate JSON outputs. So, it would be best to use a more powerful model (like Anthropic or OpenAIâ€™s most recent models) to get the best result.\n",
    "```\n",
    "This wrapper uses the Pydantic library to define and validate data structures in Python. It allows determining the expected output structure, including its name, type, and description. For instance, a variable must hold multiple suggestions, like a list, in the thesaurus application. This is achieved by creating a class that inherits the Pydanticâ€™s `BaseModel class`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Set the OPENAI_API_KEY environment variable with your API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_custom_utils.helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-3.5-turbo'\n",
    "temperature = 0.0\n",
    "model = ChatOpenAI(model_name=model_name, temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pydantic OutputParser Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired data structure.\n",
    "class Suggestions(BaseModel):\n",
    "    words: List[str] = Field(description=\"list of substitue words based on context\")\n",
    "\n",
    "    # Throw error in case of recieving a numbered-list from API\n",
    "    @validator('words')\n",
    "    def not_start_with_number(cls, field):\n",
    "        if field[0].isnumeric():\n",
    "            raise ValueError(\"The word can not start with numbers!\")\n",
    "        return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Suggestions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries and create the `Suggestions` schema class, which consists of two components:\n",
    "\n",
    "1.  **Expected Outputs:** Each output is defined by declaring a variable with the desired type, such as a list of strings (: List[str]) in the example code. Alternatively, it could be a single string (: str) for cases expecting a singular word or sentence as the response. Itâ€™s mandatory to provide a brief description using the Field functionâ€™s description attribute, aiding the model during inference. (An illustration of handling multiple outputs will be presented later in the book.)\n",
    "2.  **Validators:** We can declare functions to validate the formatting. For instance, the provided code has a validation to ensure the first character is not a number. The functionâ€™s name is not critical, but the @validator decorator must be applied to the variable requiring validation (e.g., @validator('words')). Note that if the variable is specified as a list, the field argument within the validator function will also be a list.\n",
    "\n",
    "We will pass the created class to the `PydanticOutputParser` wrapper to make it a `LangChain` parser object. The next step is to prepare the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Offer a list of suggestions to substitue the specified target_word based the presented context.\n",
    "{format_instructions}\n",
    "target_word={target_word}\n",
    "context={context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word=\"behaviour\"\n",
    "context=\"The behaviour of the students in the classroom was disruptive and made it difficult for the teacher to conduct the lesson.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"target_word\", \"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template variable is a string incorporating named index placeholders in the following `{variable_name}` format. The template variable defines our prompts for the model, with the anticipated formatting from the output parser and the inputs (the `{format_instructions}` placeholder will be replaced by instructions from the output parser). The `PromptTemplate` takes in the template string, specifying the type of each placeholder. These placeholders can be categorized as `input_variables`, whose values are assigned later through the `.format_prompt()` method or `partial_variables`, defined immediately.\n",
    "\n",
    "For querying models like GPT, the prompt will be passed on LangChainâ€™s OpenAI wrapper. (Itâ€™s important to set the `OPENAI_API_KEY` environment variables with your API key from OpenAI.) Setting the temperature value to 0 also ensures that the outcomes are consistent and reproducible.\n",
    "\n",
    "> ðŸ’¡The temperature value could be between 0 and 1, where a higher number means the model is more creative. Using larger value in production is a good practice for tasks requiring creative output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=model, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the LLMChain to get the AI-generated answer\n",
    "output = chain.run({\"target_word\": target_word, \"context\":context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parser objectâ€™s `parse()` function will convert the modelâ€™s string response to the format we specified. You can index through the list of words and use them in your applications. Notice the simplicity of accessing the third suggestion by calling the third index instead of dealing with a lengthy string that requires extensive preprocessing, as demonstrated in the initial example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-building-llm-powered-applications-with-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
