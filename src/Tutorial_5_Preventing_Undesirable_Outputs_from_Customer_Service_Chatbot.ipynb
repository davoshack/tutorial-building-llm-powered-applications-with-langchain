{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Preventing Undesirable Outputs from a Customer Service Chatbot\n",
    "\n",
    "• Find  [the Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2007%20-%20Guarding_Against_Undesirable_Outputs_with_the_Self_Critique_Chain_Example.ipynb)  for the real-world example at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "In this section, we will build a chatbot that can handle inquiries based on the content available on a website, such as blogs or documentation. The goal is to ensure the chatbot’s responses are appropriate and do not harm the brand’s reputation. This is particularly important when the chatbot may not find answers from its primary database.\n",
    "\n",
    "The process begins with selecting webpages that can serve as the information source (in this case, using LangChain’s documentation pages). The content from these pages is stored in the Deep Lake vector database, facilitating retrieval.\n",
    "\n",
    "First, use the `newspaper` library to extract content from each URL specified in the documents variable. Next, employ a recursive text splitter to segment the content into chunks of 1,000 characters, with a 100-character overlap between each segment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = [\n",
    "    'https://python.langchain.com/docs/get_started/introduction',\n",
    "    'https://python.langchain.com/docs/get_started/quickstart',\n",
    "    'https://python.langchain.com/docs/modules/model_io/models',\n",
    "    'https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates'\n",
    "]\n",
    "\n",
    "pages_content = []\n",
    "\n",
    "# Retrieve the Content\n",
    "for url in documents:\n",
    "    try:\n",
    "        article = newspaper.Article( url )\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        if len(article.text) > 0:\n",
    "            pages_content.append({ \"url\": url, \"text\": article.text })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Split to Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "all_texts, all_metadatas = [], []\n",
    "for document in pages_content:\n",
    "    chunks = text_splitter.split_text(document[\"text\"])\n",
    "    for chunk in chunks:\n",
    "        all_texts.append(chunk)\n",
    "        all_metadatas.append({ \"source\": document[\"url\"] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the `DeepLake` class, process records with an embedding function such as `OpenAIEmbeddings`, and store the data in the cloud using the `.add_texts()` method.\n",
    "\n",
    "First, add the `ACTIVELOOP_TOKEN` key to your environment variables as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_custom_utils.helper import get_activeloop_api_key\n",
    "ACTIVELOOP_API_KEY = get_activeloop_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, execute the subsequent code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = ACTIVELOOP_API_KEY\n",
    "my_activeloop_dataset_name = \"langchain_course_constitutional_chain\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "# Before executing the following code, make sure to have your\n",
    "# Activeloop key saved in the “ACTIVELOOP_TOKEN” environment variable.\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_texts(all_texts, all_metadatas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-building-llm-powered-applications-with-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
