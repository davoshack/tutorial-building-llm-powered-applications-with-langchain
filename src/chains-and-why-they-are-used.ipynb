{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are LangChain Chains\n",
    "\n",
    "In LangChain, chains facilitate the creation of end-to-end RAG pipelines. They integrate various components into a user-friendly interface, including the model, prompt, memory, output parsing, and debugging capabilities. A chain does the following: 1) receives the user’s query as input, 2) processes the LLM’s response, and 3) returns the output to the user.\n",
    "\n",
    "To design a custom pipeline, one can extend the Chain class. An example is the LLMChain, which represents the most basic chain type in LangChain and inherits from the Chain parent class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text with LLMChain\n",
    "\n",
    "Several methods, each with a unique output format, are available for effectively using a chain. This section will create a bot to suggest contextually appropriate replacement words. The following code snippet uses the GPT-3 model via the OpenAI API. It employs the PromptTemplate feature from LangChain and unifies the process using the LLMChain class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the OPENAI_API_KEY environment variable with your API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_custom_utils.helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "\n",
    "prompt_template = \"What is a word to replace the following: {word}?\"\n",
    "\n",
    "# Set the \"OPENAI_API_KEY\" environment variable before running following line.\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "llm_chain(\"artificial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest use of the chain is the __call__ method. This method directly passes the input to the object during its initialization and returns the input variable and the model’s response, provided under the text key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to pass numerous inputs simultaneously and receive a list for each input using the `.apply()` method. The only distinction is that inputs are not included in the returned list but will be in the same order as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = [\n",
    "    {\"word\": \"artificial\"},\n",
    "    {\"word\": \"intelligence\"},\n",
    "    {\"word\": \"robot\"}\n",
    "]\n",
    "\n",
    "llm_chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.generate()` method provides a more detailed response by returning an instance of LLMResult. This instance includes additional information, such as the `finish_reason` key, which clarifies why the generation process concluded. It could indicate that the model chose to finish or exceeded the length limit. Other self-explanatory data includes the total number of used tokens and the model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.generate(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method to consider is `.predict()`, which can be used interchangeably with .`run()`. This method is particularly effective when dealing with multiple inputs for a single prompt, though it can also be utilized with a single input. The following prompt will give both the word to be substituted and the context that the model must examine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"Looking at the context of '{context}'. What is a approapriate word to replace the following: {word}?\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=prompt_template, input_variables=[\"word\", \"context\"]))\n",
    "\n",
    "llm_chain.predict(word=\"fan\", context=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model effectively recommended “Ventilator” as an appropriate replacement for the word “fan” in the context of “objects.” Additionally, when the experiment is conducted with a different context, “humans”, the suggested replacement changes to “_Admirer_”. This demonstrates the model’s ability to adapt its responses based on the specified context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.predict(word=\"fan\", context=\"humans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly pass a prompt as a string to a Chain and initialize it using the `.from_string()` function as follows: `LLMChain.from_string(llm=llm, template=template)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Looking at the context of '{context}'. What is a approapriate word to replace the following: {word}?\"\"\"\n",
    "llm_chain = LLMChain.from_string(llm=llm, template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\"\"\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=[], output_parser=output_parser))\n",
    "\n",
    "llm_chain.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain.predict_and_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Memory with ConversationalChain\n",
    "\n",
    "Depending on the application, memory is a component that enriches a chain. Using the `ConversationalBufferMemory` class, LangChain provides a `ConversationalChain` to track past queries and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "template = \"\"\"List all possible words as substitute for 'artificial' as comma separated.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "{input}\"\"\"\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
    "    memory=ConversationBufferMemory())\n",
    "\n",
    "conversation.predict_and_parse(input=\"Answer briefly. write the first 3 options.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ask it to return the following four replacement words, it uses the memory to keep the context of the current task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict_and_parse(input=\"And the next 4?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating Chains with SequentialChain\n",
    "\n",
    "Another helpful feature is using a sequential chain that concatenates multiple chains into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import SimpleSequentialChain\n",
    "# overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SimpleSequentialChain` will start running each chain from the first index and pass its response to the next one in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Chains\n",
    "\n",
    "Debugging LangChain chains helps identify errors, optimize performance, and ensure smooth interactions between components, improving the reliability of complex language model workflows.\n",
    "\n",
    "Setting the `verbose` option to `True` allows you to see the inner workings of any chain. As shown in the code below, the chain will return the initial prompt and the output. The application determines the output. If there are more steps, it may provide more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate(template=template, input_variables=[\"history\", \"input\"], output_parser=output_parser),\n",
    "    memory=ConversationBufferMemory(),\n",
    "    verbose=True)\n",
    "\n",
    "conversation.predict_and_parse(input=\"Answer briefly. write the first 3 options.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Chain\n",
    "\n",
    "LangChain offers a range of predefined chains tailored for specific tasks, including the `TransformChain`, `LLMCheckerChain`, `LLMSummarizationCheckerChain`, and `OpenAPIEndpointChain`. These chains share common characteristics discussed earlier. Additionally, LangChain enables the creation of custom chains to meet unique requirements. This section focuses on constructing a custom chain to provide a word’s meaning and suggest an alternative.\n",
    "\n",
    "The process begins by creating a new class that inherits its capabilities from the Chain class. To adapt this class to a specific task, it is necessary to implement three essential methods: the `input_keys` and `output_keys` methods to inform the model of the expected inputs and outputs and the `_call` for executing each link in the chain and integrating their outputs into a coherent result.\n",
    "\n",
    "The following example shows how to create a custom chain that concatenates the outputs of two separate chains and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "class ConcatenateChain(Chain):\n",
    "    chain_1: LLMChain\n",
    "    chain_2: LLMChain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        # Union of the input keys of the two chains.\n",
    "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
    "        return list(all_input_vars)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['concat_output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        output_1 = self.chain_1.run(inputs)\n",
    "        output_2 = self.chain_2.run(inputs)\n",
    "        return {'concat_output': output_1 + output_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `LLMChain` class, we’ll declare each chain independently and use our custom chain `ConcatenateChain` to combine the results of `chain_1` and `chain_2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is the meaning of the following word '{word}'?\",\n",
    ")\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
    "\n",
    "prompt_2 = PromptTemplate(\n",
    "    input_variables=[\"word\"],\n",
    "    template=\"What is a word to replace the following: {word}?\",\n",
    ")\n",
    "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
    "\n",
    "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
    "concat_output = concat_chain.run(\"artificial\")\n",
    "print(f\"Concatenated output:\\n{concat_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-building-llm-powered-applications-with-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
