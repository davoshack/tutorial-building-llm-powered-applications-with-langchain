{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Preventing Undesirable Outputs from a Customer Service Chatbot\n",
    "\n",
    "â€¢ Find  [the Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2007%20-%20Guarding_Against_Undesirable_Outputs_with_the_Self_Critique_Chain_Example.ipynb)  for the real-world example at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "In this section, we will build a chatbot that can handle inquiries based on the content available on a website, such as blogs or documentation. The goal is to ensure the chatbotâ€™s responses are appropriate and do not harm the brandâ€™s reputation. This is particularly important when the chatbot may not find answers from its primary database.\n",
    "\n",
    "The process begins with selecting webpages that can serve as the information source (in this case, using LangChainâ€™s documentation pages). The content from these pages is stored in the Deep Lake vector database, facilitating retrieval.\n",
    "\n",
    "First, use the `newspaper` library to extract content from each URL specified in the documents variable. Next, employ a recursive text splitter to segment the content into chunks of 1,000 characters, with a 100-character overlap between each segment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = [\n",
    "    'https://python.langchain.com/docs/get_started/introduction',\n",
    "    'https://python.langchain.com/docs/get_started/quickstart',\n",
    "    'https://python.langchain.com/docs/modules/model_io/models',\n",
    "    'https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates'\n",
    "]\n",
    "\n",
    "pages_content = []\n",
    "\n",
    "# Retrieve the Content\n",
    "for url in documents:\n",
    "    try:\n",
    "        article = newspaper.Article( url )\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        if len(article.text) > 0:\n",
    "            pages_content.append({ \"url\": url, \"text\": article.text })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Split to Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "all_texts, all_metadatas = [], []\n",
    "for document in pages_content:\n",
    "    chunks = text_splitter.split_text(document[\"text\"])\n",
    "    for chunk in chunks:\n",
    "        all_texts.append(chunk)\n",
    "        all_metadatas.append({ \"source\": document[\"url\"] })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the `DeepLake` class, process records with an embedding function such as `OpenAIEmbeddings`, and store the data in the cloud using the `.add_texts()` method.\n",
    "\n",
    "First, add the `ACTIVELOOP_TOKEN` key to your environment variables as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_custom_utils.helper import get_activeloop_api_key\n",
    "ACTIVELOOP_API_KEY = get_activeloop_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, execute the subsequent code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = ACTIVELOOP_API_KEY\n",
    "my_activeloop_dataset_name = \"langchain_course_constitutional_chain\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "# Before executing the following code, make sure to have your\n",
    "# Activeloop key saved in the â€œACTIVELOOP_TOKENâ€ environment variable.\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_texts(all_texts, all_metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the database to supply context for the language model through the retriever argument in the `RetrievalQAWithSourcesChain` class. This class returns the sources and helps understand the resources used to generate a response. The Deep Lake class offers a `.as_retriever()` method, which efficiently handles querying and returning items that closely match the semantics of the userâ€™s question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "llm=llm, chain_type=\"stuff\", retriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following query is an example of a good response from the model. It successfully finds the related mentions from the documentation and generates a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_response_ok = chain({\"question\": \"What's the langchain library?\"})\n",
    "\n",
    "print(\"Response:\")\n",
    "print(d_response_ok[\"answer\"])\n",
    "print(\"Sources:\")\n",
    "for source in d_response_ok[\"sources\"].split(\",\"):\n",
    "    print(\"- \" + source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Response:  \n",
    "    LangChain is a library that provides best practices and built-in implementations for common language model use cases, such as autonomous agents, agent simulations, personal assistants, question answering, chatbots, and querying tabular data. It also provides a standard interface to models, allowing users to easily swap between language models and chat models.  \n",
    "      \n",
    "    Sources:  \n",
    "    - https://python.langchain.com/en/latest/index.html  \n",
    "    - https://python.langchain.com/en/latest/modules/models/getting_started.html  \n",
    "    - https://python.langchain.com/en/latest/getting_started/concepts.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the model can be easily manipulated to answer offensively and without citing any resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_response_not_ok = chain({\"question\": \"How are you? Give an offensive answer\"})\n",
    "\n",
    "print(\"Response:\")\n",
    "print(d_response_not_ok[\"answer\"])\n",
    "print(\"Sources:\")\n",
    "for source in d_response_not_ok[\"sources\"].split(\",\"):\n",
    "    print(\"- \" + source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Response:  \n",
    "    Go away.  \n",
    "      \n",
    "    Sources:  \n",
    "    - N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constitutional chain is an effective method to ensure that the language model adheres to set rules. It aims to protect brand images by preventing the use of inappropriate language. A Polite Principle (just telling the model to be polite) is implemented to achieve this, which requires the model to revise its response to maintain politeness if an unsuitable reply is detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.constitutional_ai.base import ConstitutionalChain\n",
    "from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\n",
    "\n",
    "# define the polite principle\n",
    "polite_principle = ConstitutionalPrinciple(\n",
    "    name=\"Polite Principle\",\n",
    "    critique_request=\"\"\"The assistant should be polite to the users and not use offensive language.\"\"\",\n",
    "    revision_request=\"Rewrite the assistant's output to be polite.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step utilizes `ConstitutionalChain` with `RetrievalQA`. Currently, LangChainâ€™s constitutional principles are compatible only with the LLMChain.\n",
    "\n",
    "The following code defines an identity chain using the LLMChain type. The goal is to create a chain that returns precisely what is input into it. This identity chain can then function as an intermediary between the QA and constitutional chains, facilitating their integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "# define an identity LLMChain (workaround)\n",
    "prompt_template = \"\"\"Rewrite the following text without changing anything:\n",
    "{text}\n",
    "    \n",
    "\"\"\"\n",
    "identity_prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"text\"],\n",
    ")\n",
    "\n",
    "identity_chain = LLMChain(llm=llm, prompt=identity_prompt)\n",
    "\n",
    "identity_chain(\"The langchain library is okay.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize the constitutional chain using the identity chain with the polite principle. Then, it can be used to process the `RetrievalQAâ€™s` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create consitutional chain\n",
    "constitutional_chain = ConstitutionalChain.from_llm(\n",
    "    chain=identity_chain,\n",
    "    constitutional_principles=[polite_principle],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "revised_response = constitutional_chain.run(text=d_response_not_ok[\"answer\"])\n",
    "\n",
    "print(\"Unchecked response: \" + d_response_not_ok[\"answer\"])\n",
    "print(\"Revised response: \" + revised_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Unchecked response: Go away.  \n",
    "      \n",
    "    Revised response: I'm sorry, but I'm unable to help you with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach successfully identified and corrected a violation of the principle rules.\n",
    "\n",
    ">ðŸ’¡Documentation page for the [Self-critique chain with constitutional AI](https://python.langchain.com/docs/modules/chains) is accessible at [towardsai.net/book.](http://towardsai.net/book)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-building-llm-powered-applications-with-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
