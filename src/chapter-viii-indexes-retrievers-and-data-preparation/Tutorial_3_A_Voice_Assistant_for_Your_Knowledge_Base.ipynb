{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Voice Assistant for Your Knowledge Base\n",
    "\n",
    "• Find the link to the “[JarvisBase](https://github.com/peterw/JarvisBase)” GitHub repository for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "This tutorial focuses on voice capabilities. In this project, we will create a voice assistant that integrates OpenAI’s Whisper to convert voice inputs into text. After the transcription is complete, voice responses will be generated using Eleven Labs, a company renowned for its high-quality text-to-speech API that adeptly captures emotion and tone. Using this API will ensure that the voice assistant can communicate with users in a clear and natural tone.\n",
    "\n",
    "At the heart of this project is a question-answering system. When a question is asked, the system retrieves relevant documents from this database. These documents and the question are then processed by a large language model (LLM). The LLM utilizes this information to formulate an appropriate response.\n",
    "\n",
    "The project includes the Streamlit service to create an interactive user interface (UI), enhancing user interaction with the assistant. This basic frontend allows users to ask questions using either natural language or voice and generates responses in both text and audio formats.\n",
    "\n",
    "![image](../images/jarvis-base.jpg)\n",
    "\n",
    "Start by installing the necessary libraries for this project. While it’s best to use the most recent versions of these packages for the best results, the provided code was used with specific versions. They can be installed using the pip packages manager. A link to this requirement file is accessible at [towardsai.net/book](http://towardsai.net/book)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the API keys and tokens. They need to be set in the environment variable as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_custom_utils.helper import get_openai_api_key, get_activeloop_api_key, get_eleven_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "ACTIVELOOP_API_KEY = get_activeloop_api_key()\n",
    "ELEVEN_API_KEY = get_eleven_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Content from Hugging Face Hub\n",
    "\n",
    "We’ll begin by gathering documents from the Hugging Face Hub. These articles will form the foundation of our voice assistant’s knowledge base. We will use web scraping methods to collect relevant knowledge documents.\n",
    "\n",
    "Let’s look at and run the script.py file.\n",
    "\n",
    "Import the required modules, load environment variables, and establish the path for Deep Lake. It also creates an instance of OpenAIEmbeddings, which will be used later to embed the scraped articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "import re\n",
    "\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = \"<YOUR-ACTIVELOOP-ORG-ID>\"\n",
    "my_activeloop_dataset_name = \"langchain_course_jarvis_assistant\"\n",
    "dataset_path= 'hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}'\n",
    "\n",
    "embeddings =  OpenAIEmbeddings(model_name=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile a list of relative URLs that lead to knowledge documents from the Hugging Face Hub. To do this, define the function `get_documentation_urls()` and attach these relative URLs to the base URL of the Hugging Face Hub using another function, `construct_full_url()`, effectively establishing full URLs that can be accessed directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documentation_urls():\n",
    "    # List of relative URLs for Hugging Face documentation pages,\n",
    "    # commented a lot of these because it would take too long to scrape\n",
    "    # all of them\n",
    "    return [\n",
    "            '/docs/huggingface_hub/guides/overview',\n",
    "            '/docs/huggingface_hub/guides/download',\n",
    "            '/docs/huggingface_hub/guides/upload',\n",
    "            '/docs/huggingface_hub/guides/hf_file_system',\n",
    "            '/docs/huggingface_hub/guides/repository',\n",
    "            '/docs/huggingface_hub/guides/search',\n",
    "            # You may add additional URLs here or replace all of them\n",
    "    ]\n",
    "\n",
    "def construct_full_url(base_url, relative_url):\n",
    "    # Construct the full URL by appending the relative URL to the base URL\n",
    "    return base_url + relative_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script compiles the gathered content from various URLs. This is executed by the `scrape_all_content()` function, which invokes the `scrape_page_content()` function for each URL. Next, the resulting text is stored in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_content(url):\n",
    "    # Send a GET request to the URL and parse the HTML response using\n",
    "    # BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Extract the desired content from the page (in this case, the body text)\n",
    "    text=soup.body.text.strip()\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b-\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', text)\n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def scrape_all_content(base_url, relative_urls, filename):\n",
    "    # Loop through the list of URLs, scrape content and add it to the\n",
    "    # content list\n",
    "    content = []\n",
    "    for relative_url in relative_urls:\n",
    "        full_url = construct_full_url(base_url, relative_url)\n",
    "        scraped_content = scrape_page_content(full_url)\n",
    "        content.append(scraped_content.rstrip('\\n'))\n",
    "\n",
    "    # Write the scraped content to a file\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        for item in content:\n",
    "            file.write(\"%s\\n\" % item)\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Splitting Texts\n",
    "\n",
    "To prepare the gathered text into our vector database, the content is first retrieved from the file using the `load_docs()` function, which separates it into distinct documents. These documents are then divided into smaller segments using the `split_docs()` function.\n",
    "\n",
    "The command `text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)` initializes a text splitter designed to segment the text into character-based chunks. It divides the documents into sections of roughly 1000 characters with no overlapping content in the consecutive sections within docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load documents from a file\n",
    "def load_docs(root_dir, filename):\n",
    "    # Create an empty list to hold the documents\n",
    "    docs = []\n",
    "    try:\n",
    "        # Load the file using the TextLoader class and UTF-8 encoding\n",
    "        loader = TextLoader(os.path.join(\n",
    "            root_dir, filename), encoding='utf-8')\n",
    "        # Split the loaded file into separate documents and add them to the list\n",
    "        # of documents\n",
    "        docs.extend(loader.load_and_split())\n",
    "    except Exception as e:\n",
    "        # If an error occurs during loading, ignore it and return an empty list\n",
    "        # of documents\n",
    "        pass\n",
    "    # Return the list of documents\n",
    "    return docs\n",
    "  \n",
    "def split_docs(docs):\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding and Storing in Deep Lake\n",
    "\n",
    "The next phase is embedding the articles and storing them in Deep Lake.\n",
    "\n",
    "The following code sets up a Deep Lake instance, specifying the dataset path and the **OpenAIEmbeddings** function as an embedding function to use. The **OpenAIEmbeddings** function transforms the text segments into their embedding vectors, a format compatible with the vector database. With the `.add_documents` method, the texts are processed and stored within the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main function\n",
    "def main():\n",
    "    base_url = 'https://huggingface.co'\n",
    "    # Set the name of the file to which the scraped content will be saved\n",
    "    filename='content.txt'\n",
    "    # Set the root directory where the content file will be saved\n",
    "    root_dir ='./'\n",
    "    relative_urls = get_documentation_urls()\n",
    "    # Scrape all the content from the relative URLs and save it to the content\n",
    "    # file\n",
    "    content = scrape_all_content(base_url, relative_urls, filename)\n",
    "    # Load the content from the file\n",
    "    docs = load_docs(root_dir, filename)\n",
    "    # Split the content into individual documents\n",
    "    texts = split_docs(docs)\n",
    "    # Create a DeepLake database with the given dataset path and embedding\n",
    "    # function\n",
    "    db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "    # Add the individual documents to the database\n",
    "    db.add_documents(texts)\n",
    "    # Clean up by deleting the content file\n",
    "    os.remove(filename)\n",
    "\n",
    "# Call the main function if this script is being run as the main program\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps are organized within the main function. It establishes the required parameters, activates the outlined functions, and manages the entire procedure, from scraping web content to integrating it into the Deep Lake database. It also removes the content file, ensuring a clean workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Voice Assistant\n",
    "\n",
    "You can find the relevant code in the *chat.py* file within the GitHub repository. To test it out, execute streamlit run *chat.py*.\n",
    "\n",
    "The libraries used below are essential to create web applications with Streamlit. They help manage audio input, generate text responses, and efficiently access information stored in the Deep Lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "import streamlit as st\n",
    "from audio_recorder_streamlit import audio_recorder\n",
    "from elevenlabs import generate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DeepLake\n",
    "from streamlit_chat import message\n",
    "\n",
    "# Constants\n",
    "TEMP_AUDIO_PATH = \"temp_audio.wav\"\n",
    "AUDIO_FORMAT = \"audio/wav\"\n",
    "\n",
    "# Load environment variables from .env file and return the keys\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "eleven_api_key = ELEVEN_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_and_database(active_loop_data_set_path):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = DeepLake(\n",
    "        dataset_path=active_loop_data_set_path,\n",
    "        read_only=True,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, prepare the code for transcribing audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe audio using OpenAI Whisper API\n",
    "def transcribe_audio(audio_file_path, openai_key):\n",
    "    openai.api_key = openai_key\n",
    "    try:\n",
    "        with open(audio_file_path, \"rb\") as audio_file:\n",
    "            response = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "        return response[\"text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Whisper API: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transcribe an audio file into text using the OpenAI Whisper API. It requires the path of the audio file and the OpenAI key as input parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record audio using audio_recorder and transcribe using transcribe_audio\n",
    "def record_and_transcribe_audio():\n",
    "    audio_bytes = audio_recorder()\n",
    "    transcription = None\n",
    "    if audio_bytes:\n",
    "        st.audio(audio_bytes, format=AUDIO_FORMAT)\n",
    "\n",
    "        with open(TEMP_AUDIO_PATH, \"wb\") as f:\n",
    "            f.write(audio_bytes)\n",
    "\n",
    "        if st.button(\"Transcribe\"):\n",
    "            transcription = transcribe_audio(TEMP_AUDIO_PATH, openai.api_key)\n",
    "            os.remove(TEMP_AUDIO_PATH)\n",
    "            display_transcription(transcription)\n",
    "\n",
    "    return transcription\n",
    "\n",
    "# Display the transcription of the audio on the app\n",
    "def display_transcription(transcription):\n",
    "    if transcription:\n",
    "        st.write(f\"Transcription: {transcription}\")\n",
    "        with open(\"audio_transcription.txt\", \"w+\") as f:\n",
    "            f.write(transcription)\n",
    "    else:\n",
    "        st.write(\"Error transcribing audio.\")\n",
    "\n",
    "# Get user input from Streamlit text input field\n",
    "def get_user_input(transcription):\n",
    "    return st.text_input(\"\", value=transcription if transcription else \"\",\n",
    "    key=\"input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code allows users to record audio straight from the program. The recorded audio is transcribed into text using the Whisper API and presented on the application. The user will be notified if an error occurs during the transcription process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the database for a response based on the user's query\n",
    "def search_db(user_input, db):\n",
    "    print(user_input)\n",
    "    retriever = db.as_retriever()\n",
    "    retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "    retriever.search_kwargs['fetch_k'] = 100\n",
    "    retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "    retriever.search_kwargs['k'] = 4\n",
    "    model = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
    "    qa = RetrievalQA.from_llm(model, retriever=retriever,\n",
    "    return_source_documents=True)\n",
    "    return qa({'query': user_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code searches the vector database for responses most relevant to the user’s query. Initially, it transforms the database into a retriever, a mechanism designed to identify the closest embeddings in the vector space. The process involves setting various search parameters, such as the metric for measuring distances within the embedding space, the initial number of documents to retrieve, the decision to employ maximal marginal relevance for balancing the diversity and relevance of outcomes, and the total number of results to be returned. Subsequently, the results are processed through a language model, GPT-3.5 Turbo, in this case, to formulate the most suitable response to the user’s inquiry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display conversation history using Streamlit messages\n",
    "def display_conversation(history):\n",
    "    for i in range(len(history[\"generated\"])):\n",
    "        message(history[\"past\"][i], is_user=True, key=str(i) + \"_user\")\n",
    "        message(history[\"generated\"][i], key=str(i))\n",
    "        #Voice using Eleven API\n",
    "        voice= \"Bella\"\n",
    "        text= history[\"generated\"][i]\n",
    "        audio = generate(text=text, voice=voice, api_key=eleven_api_key)\n",
    "        st.audio(audio, format='audio/mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Interaction\n",
    "\n",
    "The next stage is user interaction. The voice assistant is coded to receive requests through voice recordings or text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to run the app\n",
    "def main():\n",
    "    # Initialize Streamlit app with a title\n",
    "    st.write(\"# JarvisBase 🧙\")\n",
    "   \n",
    "    # Load embeddings and the DeepLake database\n",
    "    db = load_embeddings_and_database(dataset_path)\n",
    "\n",
    "    # Record and transcribe audio\n",
    "    transcription = record_and_transcribe_audio()\n",
    "\n",
    "    # Get user input from text input or audio transcription\n",
    "    user_input = get_user_input(transcription)\n",
    "\n",
    "    # Initialize session state for generated responses and past messages\n",
    "    if \"generated\" not in st.session_state:\n",
    "        st.session_state[\"generated\"] = [\"I am ready to help you\"]\n",
    "    if \"past\" not in st.session_state:\n",
    "        st.session_state[\"past\"] = [\"Hey there!\"]\n",
    "        \n",
    "    # Search the database for a response based on user input and update the\n",
    "    # session state\n",
    "    if user_input:\n",
    "        output = search_db(user_input, db)\n",
    "        print(output['source_documents'])\n",
    "        st.session_state.past.append(user_input)\n",
    "        response = str(output[\"result\"])\n",
    "        st.session_state.generated.append(response)\n",
    "\n",
    "    #Display conversation history using Streamlit messages\n",
    "    if st.session_state[\"generated\"]:\n",
    "        display_conversation(st.session_state)\n",
    "\n",
    "# Run the main function when the script is executed\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code serves as the core functionality of the application. It initializes the Streamlit application and loads the Deep Lake vector database and embeddings. The application offers two modes for user input: textual input or an audio recording, which is transcribed afterward.\n",
    "\n",
    "The application tracks previous user inputs and responses using a session state to maintain continuity. Upon receiving new input from the user, it searches the database to find the most appropriate response, updating the session state accordingly.\n",
    "\n",
    "Finally, the application shows the complete conversation history, encompassing user inputs and chatbot responses. For voice inputs, the chatbot’s responses are also presented in an audio format, leveraging the Eleven Labs API.\n",
    "\n",
    "To run the whole application, execute the following command in your terminal: `streamlit run chat.py`\n",
    "\n",
    "When you execute your program with the Streamlit command, it will launch a local web server and provide you with a URL where your application can be browsed.\n",
    "\n",
    "Your application will run as long as the command in your terminal is active, and it will terminate when you stop the command `(ctrl+C)` or close the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying Out the UI\n",
    "Now, test the Streamlit app!\n",
    "\n",
    "By clicking on the microphone icon, your microphone will be active for seconds, and you can ask a question. Let’s try “How do I search for models in the Hugging Face Hub?”.\n",
    "\n",
    "![image](../images/jarvis-flow-1.jpg)\n",
    "\n",
    "After a few seconds, the app will show an audio player to listen to your registered audio. You may then click on the “Transcribe” button.\n",
    "\n",
    "This button will invoke a call to the Whisper API and transcribe your audio. The produced text will be pasted to the chat text entry:\n",
    "\n",
    "![image](../images/jarvis-flow-2.jpg)\n",
    "\n",
    "Here, the Whisper API didn’t perfectly transcribe “Hugging Face” correctly and instead wrote “Huggy Face.” But let’s see if our LLM can still understand the query and give it an appropriate answer by leveraging the knowledge documents stored in Deep Lake.\n",
    "\n",
    "After a few more seconds, the underlying chat will be populated with your audio transcription, along with the chatbot’s textual response and its audio version, generated by calling the ElevenLabs API. As we can see, the LLM could understand that “Huggy Face” was a misspelling and was still able to give an appropriate answer.\n",
    "\n",
    "![image](../images/jarvis-flow-3.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-building-llm-powered-applications-with-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
