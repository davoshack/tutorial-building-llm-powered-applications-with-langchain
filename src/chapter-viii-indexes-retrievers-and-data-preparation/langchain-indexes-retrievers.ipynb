{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain’s Indexes and Retrievers\n",
    "\n",
    "As seen earlier, an index in LangChain is a ***data structure that organizes and stores data to facilitate quick and efficient searches***. A retriever effectively uses this index to find and provide relevant data in response to specific queries. LangChain’s **indexes** and **retrievers** provide modular, adaptable, and customizable options for ***handling unstructured data with LLMs***. The primary index types in LangChain are based on **vector databases**, mainly emphasizing indexes using **embeddings**.\n",
    "\n",
    "The role of retrievers is ***to extract relevant documents for integration into language model prompts***. In LangChain, a retriever employs a `get_relevant_documents` method, taking a query string as input and generating a list of documents that are relevant to that query.\n",
    "\n",
    "Let’s see how they work with a practical application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# text to write to a local file\n",
    "# taken from https://www.theverge.com/2023/3/14/23639313/google-ai-language-model-palm-api-challenge-openai\n",
    "text =\"\"\" Google opens up its AI language model PaLM to challenge OpenAI and GPT-3 Google offers developers access to one of its most advanced AI language models: PaLM. The search giant is launching an API for PaLM alongside a number of AI enterprise tools it says will help businesses \"generate text, images, code, videos, audio, and more from simple natural language prompts.\"\n",
    "\n",
    "PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or Meta's Llama family of models. Google first announced PaLM in April 2022. Like other LLMs, PaLM is a flexible system that can potentially carry out all sorts of text generation and editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for example, or you could use it for tasks like summarizing text or even writing code. (It's similar to features Google also announced today for its Workspace apps like Google Docs and Gmail.)\n",
    "\"\"\"\n",
    "\n",
    "# write text to local file\n",
    "with open(\"my_file.txt\", \"w\") as file:\n",
    "    file.write(text)\n",
    "\n",
    "# use TextLoader to load text from local file\n",
    "loader = TextLoader(\"my_file.txt\")\n",
    "docs_from_file = loader.load()\n",
    "\n",
    "print(len(docs_from_file))\n",
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `CharacterTextSplitter` to split the documents into text snippets called “chunks.” `chunk_overlap` is the number of characters that overlap between two consecutive chunks. It preserves context and improves coherence by ensuring that important information is not cut off at the boundaries of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# create a text splitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "\n",
    "# split documents into chunks\n",
    "docs = text_splitter.split_documents(docs_from_file)\n",
    "\n",
    "print(len(docs))\n",
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a **vector embedding** for each text snippet. These embeddings allow us to effectively search for documents or portions of documents that relate to our query by examining their semantic similarities.\n",
    "\n",
    "Here, we chose ***OpenAI’s embedding*** model to create the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the \"OPENAI_API_KEY\" environment variable.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to set up a vector store to create those embeddings. A **vector store** is a system that stores embeddings, allowing us to query them. In this example, we will use **Deep Lake**, a cloud-based vector database, but others like  [Chroma DB](https://www.trychroma.com/)  would do.\n",
    "\n",
    "Let’s create an instance of a **Deep Lake** dataset and the embeddings by providing the embedding_function.\n",
    "\n",
    "You will need a free Activeloop account to follow along:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_custom_utils.helper import get_openai_api_key, get_activeloop_api_key \n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "ACTIVELOOP_API_KEY = get_activeloop_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "# Before executing the following code, make sure to have your\n",
    "# Activeloop key saved in the \"ACTIVELOOP_TOKEN\" environment variable.\n",
    "\n",
    "# create Deep Lake dataset\n",
    "# TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_org_id = ACTIVELOOP_API_KEY\n",
    "my_activeloop_dataset_name = \"langchain_course_indexers_retrievers\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to our Deep Lake dataset\n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a LangChain retriever by calling the `.as_retriever()` method on your **vector store instance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriever from db\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the retriever, we can use the `RetrievalQA` class to define a question answering chain using an external data source and start with `question-answering`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# create a retrieval chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query our document about a specific topic found in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How Google plans to challenge OpenAI?\"\n",
    "response = qa_chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see something like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Google plans to challenge OpenAI by offering access to its AI language model PaLM, which is similar to OpenAI's GPT series and Meta's Llama family of models. PaLM is a large language model that can be used for tasks like summarizing text or writing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In creating the retriever stages, we set the `chain_type` to “stuff.” This is the most straightforward document chain (“stuff” as in “to stuff” or “to fill”). It takes a list of documents, inserts them all into a prompt, and passes that prompt to an LLM. This approach is only efficient with shorter documents due to the context length limitations of most LLMs.\n",
    "\n",
    "The process also involves conducting a similarity search using embeddings to find documents relevant to the query and can be used as context for the LLM. While this might appear limited in scope with a single document, its effectiveness is enhanced when dealing with multiple documents segmented into chunks. We supply the LLM with the relevant information within its context size by selecting the most relevant documents based on semantic similarity.\n",
    "\n",
    "The effectiveness of this approach in enhancing the language comprehension of large language models is underscored by the retriever’s ability to pinpoint documents closely related to a user’s query in the embedding space.\n",
    "\n",
    "It is important to note that this method poses a notable challenge, especially when dealing with a more extensive data set. In the example, the text was divided into equal parts, 200 characters long, which resulted in both relevant and irrelevant text being presented in response to a user’s query.\n",
    "\n",
    "Incorporating unrelated content in the LLM prompt can be problematic because it may distract the LLM from focusing on essential details and it consumes space in the prompt that could be allocated to more relevant information.\n",
    "\n",
    "A `DocumentCompressor` addresses this issue. Instead of immediately returning retrieved documents as-is, it compresses them so that only the information relevant to the query is returned. “Compressing” here refers to using an LLM to rewrite the retrieved chunk so that it contains only information relevant to the query. This way, the chunks are smaller, and more chunks can be used as contextual information to generate the final answer.\n",
    "\n",
    "`The ContextualCompressionRetriever` serves as a wrapper that combines a base retriever with a `DocumentCompressor`, ensuring that only the most pertinent segments of the documents retrieved by the base retriever are used.\n",
    "\n",
    "The `LLMChainExtractor` class is a `DocumentCompressor` that uses an LLM chain to extract relevant parts of documents.\n",
    "\n",
    "The following example demonstrates the application of the `ContextualCompressionRetriever` with the `LLMChainExtractor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# create GPT3 wrapper\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# create compressor for the retriever\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `compression_retriever` is created, we can retrieve the relevant compressed documents for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving compressed documents\n",
    "retrieved_docs = compression_retriever.get_relevant_documents(\n",
    "    \"How Google plans to challenge OpenAI?\"\n",
    ")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an output like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Google is offering developers access to one of its most advanced AI language models: PaLM. The search giant is launching an API for PaLM alongside a number of AI enterprise tools it says will help businesses \"generate text, images, code, videos, audio, and more from simple natural language prompts.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compressors try to simplify the process by sending  **only essential**  data to the LLM. This also allows you to provide more information to the LLM. Letting the compressors handle precision during the initial retrieval step will allow you to focus on recall (for example, by increasing the number of documents returned).\n",
    "\n",
    "We saw how to create a retriever from a .txt file; however, data can come in different types. The LangChain framework offers diverse classes that enable data to be loaded from multiple sources, including PDFs, URLs, and Google Drive, among others, which we will explore next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "\n",
    "Data ingestion can be simplified with various data loaders, each with its own specialization. The `TextLoader` from `LangChain` excels at handling plain text files. The `PyPDFLoader` is optimized for PDF files, allowing easy access to the content. The `SeleniumURLLoader` is the go-to tool for web-based data, notably HTML documents from URLs that require JavaScript rendering. The `GoogleDriveLoader` integrates seamlessly with Google Drive, allowing for data import from Google Docs or entire folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('file_path.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">💡You can use the encoding argument to change the encoding type. (For example: encoding=\"ISO-8859-1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Loading Data from PDF Files\n",
    "\n",
    "The `PyPDFLoader` class can import PDF files and create a list of `LangChain` documents. Each document in this array contains the content and metadata of a single page, including the page number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a code snippet to load and split a PDF file using `PyPDFLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"example_data/layout-parser-paper.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from Webpages\n",
    "\n",
    "The `SeleniumURLLoader` class in `LangChain` provides a user-friendly solution for importing HTML documents from URLs that require JavaScript rendering.\n",
    "\n",
    ">The code examples provided have been tested with the unstructured and selenium libraries, versions 0.7.7 and 4.10.0, respectively. You are encouraged to install the most recent versions for optimal performance and features in your application and keep these versions for output consistency in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the `SeleniumURLLoader` class by providing a list of URLs to load, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=TFa539R09EQ&t=139s\",\n",
    "    \"https://www.youtube.com/watch?v=6Zv6A_9urh4&t=112s\"\n",
    "]\n",
    "\n",
    "loader = SeleniumURLLoader(urls=urls)\n",
    "data = loader.load()\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SeleniumURLLoader` class in `LangChain` offers several attributes, such as the URLs (List[str]) to access a list of URLs, `continue_on_failure (bool, default=True)` to determine whether the loader should continue processing other URLs in case of a failure, browser (str, default=“chrome”) to select the browser (Chrome or Firefox) for loading the URLs, executable_path (Optional[str], default=None) to determine the path to the browser’s executable file, and headless (bool, default=True) to specify whether the browser should operate in headless mode, meaning it runs without a visible user interface.\n",
    "\n",
    "These attributes can be adjusted during initialization. For example, to use Firefox instead of Chrome, set the browser attribute to “firefox”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = SeleniumURLLoader(urls=urls, browser=\"firefox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `load()` method is used with the `SeleniumURLLoader` object, it returns a collection of Document instances, each containing the content fetched from the web pages. These Document instances have a page_content attribute, which includes the text extracted from the HTML, and a metadata attribute that stores the source URL.\n",
    "\n",
    "The `SeleniumURLLoader` class might operate slower than other loaders because it initializes a browser instance for each URL to render pages, especially those that require JavaScript accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">💡This approach will not work in a Google Colab notebook without further configuration, which is outside the scope of this book. Instead, try running the code directly using the Python interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from Google Drive\n",
    "\n",
    "The LangChain `GoogleDriveLoader` class can import data directly from Google Drive. It can retrieve data from a list of Google Docs document IDs or a single folder ID on Google Drive.\n",
    "\n",
    "To use the `GoogleDriveLoader`, you need to set up the necessary credentials and tokens. The loader typically looks for the credentials.json file in the ***~/.credentials/credentials.json*** directory. You can specify a different path using the `credentials_file` keyword argument. For the token, the ***token.json*** file is created automatically on the loader’s first use and follows a similar path convention.\n",
    "\n",
    "To set up the ***credentials_file***, follow these steps:\n",
    "\n",
    "1.  Create or select a Google Cloud Platform project by visiting the Google Cloud Console. Make sure billing is enabled for the project.\n",
    "2.  Activate the Google Drive API from the Google Cloud Console dashboard and click “Enable”.\n",
    "3.  Follow the steps to set up a service account via the Service Accounts page in the Google Cloud Console.\n",
    "4.  Assign the necessary roles to the service account. Roles like “Google Drive API - Drive File Access” and “Google Drive API - Drive Metadata Read/Write Access” might be required, depending on your specific use case.\n",
    "5.  Navigate to the “Actions” menu next to it, select “Manage keys,” then click “Add Key” and choose “JSON” as the key type. This will generate a JSON key file and download it to your computer, which will be used as your credentials_file.\n",
    "6.  Retrieve the folder or document ID identified at the end of the URL like this:\n",
    "    \n",
    "    – Folder: https://drive.google.com/drive/u/0/folders/{folder_id}\n",
    "    \n",
    "    – Document: https://docs.google.com/document/d/{document_id}/edit\n",
    "    \n",
    "7.  Import the `GoogleDriveLoader` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GoogleDriveLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Instantiate `GoogleDriveLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GoogleDriveLoader(\n",
    "    folder_id=\"your_folder_id\",\n",
    "    recursive=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Load the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that currently, only Google Docs are supported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitters\n",
    "\n",
    "• Find the  [Notebook](https://colab.research.google.com/github/towardsai/ragbook-notebooks/blob/main/notebooks/Chapter%2007%20-%20What_are_Text_Splitters_and_Why_They_are_Useful_.ipynb)  for this section at  [towardsai.net/book](http://towardsai.net/book).\n",
    "\n",
    "A challenge in LLMs is the limitation of input prompt size, preventing them from including all documents economically and without introducing noise. However, this can be managed using text splitters to divide documents into smaller, cohesive parts. Text splitters help break down large text documents into smaller, more digestible pieces that language models can process more effectively. It is an important tool for efficiently splitting long documents into smaller but cohesive sections to enhance the effectiveness of vector store searches.\n",
    "\n",
    "Text splitters help provide a source document to a large language model and, in turn, guide its content generation, reducing the likelihood of producing false or irrelevant information. With access to a reliable source, the LLM can deliver more accurate answers, which is particularly valuable in scenarios demanding high precision. Additionally, users can verify the information generated by cross-referencing it with the source document, ensuring reliability and correctness.\n",
    "\n",
    "However, relying on a single document can limit the scope of content generated, as the LLM is restricted to the information available in that document. If the document contains errors or biases, the LLM’s output may be misleading or incorrect. Moreover, although referencing a document can reduce the likelihood of hallucinations, it cannot entirely prevent the LLM from generating false or irrelevant content.\n",
    "\n",
    "A text splitter helps provide adequate context for the LLM to answer the query, as many small relevant segments might be more likely to match a query than a single big segment. Experimenting with different chunk sizes and overlaps can be beneficial in tailoring results to suit your specific needs.\n",
    "\n",
    "This process can become complicated when retaining the integrity of semantically connected text parts is critical.\n",
    "\n",
    "Text segmentation typically involves breaking the text into smaller, semantically meaningful units, often sentences, aggregating these smaller units into more significant segments until they reach a certain size, defined by specific criteria, and once the target size is achieved, the segment is isolated as a distinct piece. The process is repeated with some segment overlap to preserve contextual continuity.\n",
    "\n",
    "In customizing text segmentation, consider two key factors: the technique for dividing the text and the criteria used to determine the size of each final text segment.\n",
    "\n",
    "Below, we discuss the techniques and criteria commonly employed to determine the size of the chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Text by Number of Characters\n",
    "\n",
    "This splitter offers customization in two key areas: *the size of each chunk* and the *extent of overlap between chunks*. This customization balances creating manageable segments and maintaining semantic continuity across them.\n",
    "\n",
    "To begin processing documents, use the `PyPDFLoader` class. The [sample PDF file](https://github.com/towardsai/rag-ebook-files/blob/main/The%20One%20Page%20Linux%20Manual.pdf) used for this example is accessible at [towardsai.net/book](http://towardsai.net/book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"the_one_page_linux_manual.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we split the text into “chunks” of 1000 characters, overlapping 20 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "print(texts[0])\n",
    "\n",
    "print (f\"You have {len(texts)} documents\")\n",
    "print (\"Preview:\")\n",
    "print (texts[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn’t a one-size-fits-all method for segmenting text, as the effectiveness of a process can vary widely depending on the documents used. An iterative approach can determine the optimal  **chunk size**  for your project.\n",
    "\n",
    "Begin by cleaning your data and removing unnecessary elements like HTML tags from web sources. Next, experiment with different chunk sizes. Evaluate the effectiveness of each size by running queries and analyzing the results. Although this process can be time-consuming, it is an important step in achieving the best outcomes for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Text at Logical End Points\n",
    "\n",
    "The `RecursiveCharacterTextSplitter` splitter segments text into chunks based on a predefined list of strings used as separators, trying to produce chunks that are not longer than a specified max length and that follow logical sections like paragraphs or sentences.\n",
    "\n",
    "For example, the `RecursiveCharacterTextSplitter` first tries to segment the text by splitting it by paragraphs (using the “\\n\\n” separator). If a paragraph is shorter than the specified max length, it becomes a chunk. Otherwise, the `RecursiveCharacterTextSplitter` tries splitting the paragraph by newlines (using the “\\n” separator). If a line is shorter than the specified max length, it becomes a chunk. Otherwise, the next separator is used (e.g., a whitespace separator), and so on.\n",
    "\n",
    "To utilize the `RecursiveCharacterTextSplitter` splitter, create an instance with the following parameters:\n",
    "\n",
    "-   `chunk_size`: This defines the maximum size of each chunk. It is determined by the length_function, with a default value of 100.\n",
    "-   `chunk_overlap`: This specifies the maximum overlap between chunks to ensure continuity, with a default of 20.\n",
    "-   `length_function`: This calculates the length of chunks. The default is len, which counts the number of characters.\n",
    "\n",
    "Using a token counter instead of the default len function can be advantageous for specific applications, such as when working with language models with token limits. For instance, considering OpenAI’s GPT-3’s token limit of ***4096 tokens per request***, a token counter might be more effective for managing and optimizing requests.\n",
    "\n",
    "Here’s an example of how to use `RecursiveCharacterTextSplitter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"The One Page Linux Manual.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pages)\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example, we set up an instance of the `RecursiveCharacterTextSplitter` class with specific parameters, using the default character set [\"\\n\\n\", \"\\n\", \" \", \"\"] for splitting the text.\n",
    "\n",
    "Initially, the text is segmented using two newline characters (\\n\\n). If the resulting chunks exceed the desired size of 50 characters, the class attempts to divide the text using a single newline character (\\n). The result is a series of documents comprising the segmented text.\n",
    "\n",
    "To incorporate a token counter, you can create a function that determines the token count in a text and use this as the `length_function` parameter. This modification ensures that the chunk lengths are calculated based on tokens rather than character counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Text with Foreign Linguistic Structures with NLTK\n",
    "\n",
    "The `NLTKTextSplitter` splitter leverages the capabilities of the Natural Language Toolkit (NLTK) library for text segmentation. This class can make splitting decisions based on linguistic structure, thanks to many hand-written rules created by linguistics. This means it can more intelligently identify sentence boundaries, paragraph divisions, and other natural language cues that depend on the specific language used, resulting in more semantically coherent chunks of text.\n",
    "\n",
    ">💡If it is your first time using this package, you will need to install the NLTK library using !pip install -q nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "# Load a long document\n",
    "with open('/home/cloudsuperadmin/scrape-chain/langchain/LLM.txt',\n",
    "encoding= 'unicode_escape') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500)\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider using this tokenizer, particularly for foreign languages whose syntax is not based on words separated by whitespaces, such as Chinese (Mandarin and Cantonese), Japanese, and Thai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Text with Foreign Linguistic Structures with Spacy\n",
    "\n",
    "The `SpacyTextSplitter` splitter is another class for separating large text documents into smaller parts of a specific size. The `SpacyTextSplitter` splitter is an alternative to NLTK-based sentence-splitting algorithms. To use this splitter, first construct a `SpacyTextSplitter` object and set the `chunk_size` property. This size is decided by a length function, which measures the number of characters in the text by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "# Load a long document\n",
    "with open('/home/cloudsuperadmin/scrape-chain/langchain/LLM.txt',\n",
    "encoding= 'unicode_escape') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# Instantiate the SpacyTextSplitter with the desired chunk size\n",
    "text_splitter = SpacyTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "\n",
    "# Split the text using SpacyTextSplitter\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "\n",
    "# Print the first chunk\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Text with the Markdown Format\n",
    "\n",
    "The `MarkdownTextSplitter` splitter specializes in segmenting text formatted with Markdown, targeting elements like headers, code blocks, or dividers. This splitter is a specialized version of the `RecursiveCharacterSplitter` splitter, adapted for Markdown with specific separators. These separators are, by default, aligned with standard Markdown syntax but can be tailored by supplying a customized list of characters during the initialization of the `MarkdownTextSplitter` instance. The default measurement for chunk size is based on the number of characters, as determined by the provided length function. When creating an instance, an integer value can be specified to adjust the chunk size to specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "markdown_text = \"\"\"\n",
    "#\n",
    "\n",
    "# Welcome to My Blog!\n",
    "\n",
    "## Introduction\n",
    "Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python, Java, and JavaScript.\n",
    "\n",
    "Here's a list of my favorite programming languages:\n",
    "\n",
    "1. Python\n",
    "2. JavaScript\n",
    "3. Java\n",
    "\n",
    "You can check out some of my projects on [GitHub](https://github.com).\n",
    "\n",
    "## About this Blog\n",
    "In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on the latest technology trends, and occasional book reviews.\n",
    "\n",
    "Here's a small piece of Python code to say hello:\n",
    "\n",
    "\\``` python\n",
    "def say_hello(name):\n",
    "    print(f\"Hello, {name}!\")\n",
    "\n",
    "say_hello(\"John\")\n",
    "\\```\n",
    "\n",
    "Stay tuned for more updates!\n",
    "\n",
    "## Contact Me\n",
    "Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at johndoe@email.com.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "docs = markdown_splitter.create_documents([markdown_text])\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying Markdown syntax elements (e.g., headings, lists, and code blocks) enables intelligent content division based on its structural hierarchy, leading to semantically coherent segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Text with Tokens\n",
    "\n",
    "The `TokenTextSplitter` splitter offers a key advantage over splitters like the `CharacterTextSplitter` splitter by ensuring that the resulting chunks contain, at most, a specified number of tokens. This is very useful when using LLMs with limited context window since it allows us to determine the maximum number of chunks that can be inserted into the prompt without making it bigger than the maximum context size.\n",
    "\n",
    "This splitter first converts the input text into BPE (Byte Pair Encoding, seen in Chapter 2) tokens and then groups them into chunks. Then, the tokens within each chunk are converted back to their original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Load a long document\n",
    "with open('/home/cloudsuperadmin/scrape-chain/langchain/LLM.txt',\n",
    "encoding= 'unicode_escape') as f:\n",
    "    sample_text = f.read()\n",
    "\n",
    "# Initialize the TokenTextSplitter with desired chunk size and overlap\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=50)\n",
    "\n",
    "# Split into smaller chunks\n",
    "texts = text_splitter.split_text(sample_text)\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `chunk_size` parameter in `TokenTextSplitter` dictates the maximum number of BPE tokens each chunk can contain, whereas `chunk_overlap` determines the extent of token overlap between successive chunks.\n",
    "\n",
    "A potential but small downside of the `TokenTextSplitter` splitter is the increased computational effort required to convert text into BPE tokens and vice versa. For quicker and more straightforward text segmentation, the `CharacterTextSplitter` splitter may be a preferable option because it offers a more direct and less computationally intensive approach to dividing text.\n",
    "\n",
    "The above text splitters are the most commonly used approaches to splitting text. The next section focuses on how these text splitters can be leveraged to enhance your application with an example where we build a customer support Q&A chatbot powered by LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Search and Vector Embeddings\n",
    "\n",
    "OpenAI’s embedding models are versatile and can generate embeddings that we can use for similarity searches. In this section, we will use the OpenAI API to create embeddings from a collection of documents and then perform a similarity search using cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s generate embeddings for our documents and perform a similarity search.\n",
    "\n",
    "Begin by defining a list of documents as strings. This text data will be used for the subsequent steps.\n",
    "\n",
    "Next, compute the embeddings for each document using the OpenAIEmbeddings class. Set the embedding model to ***\"text-embedding-ada-002***\". This model will generate embeddings for each document, transforming them into vector representations of their semantic content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">💡Computing embeddings using a proprietary model like the \"text-embedding-ada-002\" model incurs costs due to the usage of the API. Embedding models are usually very cheap compared to using an LLM for inference, but the total cost can become significant if millions of text chunks are used. However, in this tutorial (and in all the other tutorials in this book), we will compute embeddings of a few texts, keeping the costs to a minimum. Check the OpenAI pricing page to see the current pricing for that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, convert the query string to an embedding. The query string contains the text for which we want to find the most similar document.\n",
    "\n",
    "After obtaining the embeddings for our documents and the query, calculate the cosine similarity between the query embedding and each document embedding. Cosine similarity is a widely used distance metric to assess the similarity between two vectors. In our context, it provides a series of similarity scores, each indicating how similar the query is to each document.\n",
    "\n",
    "Once we have these similarity scores, we identify the document that is most similar to the query. This is achieved by finding the index of the highest similarity score and then retrieving the corresponding document from our collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    \"The cat is on the mat.\",\n",
    "    \"There is a cat on the mat.\",\n",
    "    \"The dog is in the yard.\",\n",
    "    \"There is a dog in the yard.\",\n",
    "]\n",
    "\n",
    "# Initialize the OpenAIEmbeddings instance\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Generate embeddings for the documents\n",
    "document_embeddings = embeddings.embed_documents(documents)\n",
    "\n",
    "# Perform a similarity search for a given query\n",
    "query = \"A cat is sitting on a mat.\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Calculate similarity scores\n",
    "similarity_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "\n",
    "# Find the most similar document\n",
    "most_similar_index = np.argmax(similarity_scores)\n",
    "most_similar_document = documents[most_similar_index]\n",
    "\n",
    "print(f\"Most similar document to the query '{query}':\")\n",
    "print(most_similar_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding models can be open-source or proprietary. The choice of embedding model type depends on specific requirements, explained in the next sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-Source Embedding Models\n",
    "\n",
    "As previously discussed, embedding models belong to a specific category of machine learning models designed to transform discrete data points into vector representations. In natural language processing, these discrete elements can be words, sentences, or entire documents. The resulting vector representations, referred to as embeddings, aim to encapsulate the semantic essence of the original data. For instance, words with similar meanings, such as “cat” and “kitten,” are likely to have closely aligned embeddings. These embeddings possess high dimensionality and are utilized to capture subtle semantic differences.\n",
    "\n",
    "Open-source embedding models offer flexibility, transparency, and cost savings, allowing customization and peer-reviewed improvements. However, they may lack support, and quality can vary. Proprietary models typically provide better performance, stability, and support but come with higher costs, limited customization, and potential vendor lock-in. The choice depends on specific needs like control vs. convenience.\n",
    "\n",
    "One key advantage of using embeddings is their ability to enable mathematical operations for interpreting semantic meanings. As illustrated, a common application involves calculating the cosine similarity between two embeddings to assess the semantic closeness of associated words or documents. The following example shows how to use an open-source embedding model for this task.\n",
    "\n",
    "The example uses the model “*sentence-transformers/all-mpnet-base-v2”*, a pre-trained model for converting sentences into semantically meaningful vectors.\n",
    "\n",
    "In the model_kwargs settings, ensure the computations are carried out on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before executing the following code, install the sentence transformer library with the command *!pip install sentence_transformers===2.2.2.*\n",
    "\n",
    "***This library has robust pre-trained models specialized in generating embedding representations.***\n",
    "\n",
    "Next, define a list of documents - these are the chunks of text we wish to turn into semantic embeddings and generate the embeddings. This is accomplished by invoking the embed_documents function on the Hugging Face Embeddings instance and supplying our document list as an argument. This method goes through each document and returns a list of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
    "\n",
    "documents = [\"Document 1\", \"Document 2\", \"Document 3\"]\n",
    "doc_embeddings = hf.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These embeddings are now ready for further processing, such as classification, grouping, or similarity analysis. They reflect our original documents in a machine-readable format, allowing us to conduct complicated semantic computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohere Embeddings\n",
    "\n",
    "While the choice between closed-source and open-source embedding models will ultimately depend on your specific needs, including budget, control, and flexibility, closed-source models generally offer more accuracy, speed, and performance. Several companies offer closed-source embedding models, we chose Cohere because it is optimized for tasks like semantic search, text classification, and recommendation systems. They provide a multilingual embedding model that maps texts in different languages into the same semantic vector space, and therefore, it’s ideal in multilingual applications, especially search functionalities. This model, distinct from their English language model, employs dot product computations as a similarity metric for improved performance. The model produces 768-dimensional embeddings.\n",
    "\n",
    "An API key is required to use the Cohere API. Navigate to the  [Cohere Dashboard](https://dashboard.cohere.ai/api-keys), create a new account, or log in. Once logged in, the dashboard offers an easy-to-use interface for creating and managing API keys.\n",
    "\n",
    "After acquiring the API key, create an instance of the CohereEmbeddings class with LangChain using the “*embed-multilingual-v2.0*” model.\n",
    "\n",
    "Next, prepare a list of texts in various languages. Use the `embed_documents()` method to generate distinctive embeddings for each text.\n",
    "\n",
    "To showcase these embeddings, each text is printed with its corresponding embedding. For clarity, only the first five dimensions of each embedding are displayed.\n",
    "\n",
    "For this, the Cohere package must be installed by executing !pip install cohere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "from langchain_custom_utils.helper import get_cohere_api_key\n",
    "from langchain.embeddings import CohereEmbeddings\n",
    "\n",
    "COHERE_API_KEY = get_cohere_api_key()\n",
    "\n",
    "# Initialize the CohereEmbeddings object\n",
    "cohere = CohereEmbeddings(\n",
    "    model=\"embed-multilingual-v2.0\",\n",
    "    cohere_api_key=COHERE_API_KEY\n",
    ")\n",
    "\n",
    "# Define a list of texts\n",
    "texts = [\n",
    "    \"Hello from Cohere!\",\n",
    "    \"مرحبًا من كوهير!\",\n",
    "    \"Hallo von Cohere!\",  \n",
    "    \"Bonjour de Cohere!\",\n",
    "    \"¡Hola desde Cohere!\",\n",
    "    \"Olá do Cohere!\",  \n",
    "    \"Ciao da Cohere!\",\n",
    "    \"您好，来自 Cohere！\",\n",
    "    \"कोहेरे से नमस्ते!\"\n",
    "]\n",
    "\n",
    "# Generate embeddings for the texts\n",
    "document_embeddings = cohere.embed_documents(texts)\n",
    "\n",
    "# Print the embeddings\n",
    "for text, embedding in zip(texts, document_embeddings):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Embedding: {embedding[:5]}\")  # print first 5 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, LangChain proved helpful in simplifying the integration of an embedding model like Cohere’s multilingual embedding model into a developer’s workflow. This is one of the main advantages of working with libraries like LangChain and LlamaIndex: they make it easy to work with different types of models and switch between them without the need for big code changes.\n",
    "\n",
    "Embeddings are typically computed once and then stored in a vector database for future use. Vector databases, like most systems, can be open-source or proprietary, with respective pros and cons.\n",
    "\n",
    "We explored how vector embeddings and similarity searches can be performed using OpenAI and various embedding models. In the next section, we’ll see how LangChain, OpenAI, and Deep Lake come together to build a conversational AI system. This system efficiently retrieves relevant information and answers user queries, demonstrating the power of embeddings in real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-building-llm-powered-applications-with-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
